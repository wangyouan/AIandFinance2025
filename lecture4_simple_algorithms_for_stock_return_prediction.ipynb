{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a737010b",
   "metadata": {},
   "source": [
    "# Lecture 4: Simple Algorithms for Stock Return Prediction\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will implement and compare **four key machine learning models** for stock return prediction:\n",
    "\n",
    "1. **Linear Regression (OLS)** â€“ the baseline linear model.\n",
    "2. **Regularized Regression (Lasso, Ridge, Elastic Net)** â€“ improved generalization in high-dimensional, noisy settings.\n",
    "3. **Random Forest (RF)** â€“ nonlinear, ensemble method robust to noise.\n",
    "4. **Gradient Boosting (GBDT, XGBoost, LightGBM)** â€“ sequential ensemble method, industry standard for tabular prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Machine Learning for Stock Prediction?\n",
    "- Stock returns are **noisy** and often weakly predictable.  \n",
    "- Firm characteristics can be **high-dimensional** and correlated.  \n",
    "- Machine learning models help capture **nonlinear patterns** and avoid overfitting via regularization and ensembles.\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluation Metrics\n",
    "We will compare models using the following metrics:\n",
    "\n",
    "- **RMSE (Root Mean Squared Error):** accuracy, penalizes large errors.  \n",
    "- **MAE (Mean Absolute Error):** average error magnitude, robust to outliers.  \n",
    "- **Hit Ratio:** percentage of correct up/down predictions.  \n",
    "- **Fit Time:** computational efficiency of model training.  \n",
    "\n",
    "> In finance, **out-of-sample performance** is the gold standard â€” models must be evaluated on unseen data to ensure predictive power.\n",
    "\n",
    "---\n",
    "\n",
    "### Notebook Workflow\n",
    "1. Load and preprocess financial data.  \n",
    "2. Train models using historical features.  \n",
    "3. Evaluate models with RMSE, MAE, Hit Ratio, and training time.  \n",
    "4. Compare results to highlight **strengths and limitations** of each method.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c41f456",
   "metadata": {},
   "source": [
    "## 0. Using Pandas_datareader to Retrieve Factor Data\n",
    "\n",
    "- **Pandas_datareader** is a Python library for accessing financial and economic data from various online sources.  \n",
    "  It supports the following major databases:  \n",
    "\n",
    "  - **Tiingo**: Provides historical price data for stocks, indices, and funds. (Requires an API key, free registration available.)  \n",
    "  - **IEX (Investors Exchange)**: Offers real-time and historical data for the U.S. stock market (requires an API key).  \n",
    "  - **Alpha Vantage**: Provides real-time U.S. stock and foreign exchange data (requires an API key).  \n",
    "  - **Econdb**: Supplies macroeconomic data for major countries worldwide.  \n",
    "  - **Yahoo Finance**: Offers historical price data for stocks, indices, and funds.  \n",
    "  - **FRED (Federal Reserve Economic Data)**: Provides U.S. economic and financial time-series data.  \n",
    "  - **World Bank**: Supplies macroeconomic and development data for countries worldwide.  \n",
    "  - **OECD (Organisation for Economic Co-operation and Development)**: Provides economic and social data for member countries.  \n",
    "  - **Eurostat**: Offers official statistics from the Statistical Office of the European Union.  \n",
    "  - **Nasdaq Trader**: Provides stock data from the Nasdaq exchange.  \n",
    "  - **Stooq**: Offers global data on stocks, indices, and foreign exchange.  \n",
    "  - **TSP (Thrift Savings Plan)**: Provides data on the U.S. federal government employeesâ€™ retirement savings plan.  \n",
    "  - **Fama-French**: Offers data for the Fama-French factor models, including market, size, and value factors.  \n",
    "\n",
    "Since WRDS network restrictions prevent direct demonstration here, more details about Pandas_datareader can be found at:  \n",
    "ðŸ‘‰ https://pandas-datareader.readthedocs.io/en/latest/remote_data.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "9093671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Requirements / Imports ===\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning - scikit-learn\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pandas_datareader.data as web\n",
    "\n",
    "# Gradient Boosting - external libraries\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Timing\n",
    "import time\n",
    "\n",
    "# Display settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "plt.rcParams[\"axes.titlesize\"] = 14\n",
    "plt.rcParams[\"axes.labelsize\"] = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a66c6432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# === Data Loading from WRDS ===\n",
    "import os\n",
    "import wrds\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "wrds_user = os.getenv(\"WRDS_USER\")\n",
    "\n",
    "# Connect to WRDS\n",
    "db = wrds.Connection(wrds_username=wrds_user)\n",
    "\n",
    "# ---------------------------\n",
    "# 1. CRSP Monthly Stock Data\n",
    "# ---------------------------\n",
    "crsp_msf = db.raw_sql(\"\"\"\n",
    "    select a.permno, a.date, a.ret, a.vol, a.shrout, a.prc,\n",
    "           b.ticker, b.comnam\n",
    "    from crsp.msf as a\n",
    "    left join crsp.msenames as b\n",
    "    on a.permno=b.permno\n",
    "    and b.namedt <= a.date\n",
    "    and a.date <= b.nameendt\n",
    "    where a.date between '2010-01-01' and '2020-12-31'\n",
    "\"\"\")\n",
    "\n",
    "# Clean CRSP\n",
    "crsp_msf['date'] = pd.to_datetime(crsp_msf['date'])\n",
    "crsp_msf['ret'] = pd.to_numeric(crsp_msf['ret'], errors='coerce')\n",
    "crsp_msf['me'] = crsp_msf['prc'].abs() * crsp_msf['shrout']   # Market equity\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Fama-French Factors (Monthly)\n",
    "# ---------------------------\n",
    "ff_factors = db.raw_sql(\"\"\"\n",
    "    select date, mktrf, smb, hml, rf\n",
    "    from ff.factors_monthly\n",
    "    where date between '2010-01-01' and '2020-12-31'\n",
    "\"\"\")\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Compustat Fundamentals (Quarterly) â€” build 10 features\n",
    "# ---------------------------\n",
    "# Pull key quarterly fields from comp.fundq\n",
    "compq = db.raw_sql(\"\"\"\n",
    "    select gvkey, datadate,\n",
    "           atq,        -- total assets (quarter)\n",
    "           ceqq,       -- common equity (quarter)\n",
    "           revtq,      -- revenue (total)\n",
    "           cogsq,      -- cost of goods sold\n",
    "           ibq,        -- income before extraordinary items\n",
    "           dlttq,      -- long-term debt\n",
    "           dlcq,       -- debt in current liabilities (short-term debt)\n",
    "           capxy,      -- capital expenditures\n",
    "           cheq,       -- cash & equivalents\n",
    "           actq,       -- current assets\n",
    "           lctq        -- current liabilities\n",
    "    from comp.fundq\n",
    "    where indfmt = 'INDL'\n",
    "      and datafmt = 'STD'\n",
    "      and popsrc = 'D'\n",
    "      and consol = 'C'\n",
    "      and datadate between '2009-01-01' and '2020-12-31'\n",
    "\"\"\")\n",
    "\n",
    "# Ensure datetime and reasonable ordering\n",
    "compq['datadate'] = pd.to_datetime(compq['datadate'], errors='coerce')\n",
    "compq = compq.sort_values(['gvkey', 'datadate']).rename(columns={\"capxy\": \"capxq\"})\n",
    "\n",
    "# Convert to numeric (in case of strings), keep as floats\n",
    "num_cols = ['atq','ceqq','revtq','cogsq','ibq','dlttq','dlcq','capxq','cheq','actq','lctq']\n",
    "for c in num_cols:\n",
    "    compq[c] = pd.to_numeric(compq[c], errors='coerce')\n",
    "\n",
    "# Create a quarter-end \"date\" aligned to month-end (for later merges to monthly)\n",
    "# Using PeriodIndex to guarantee quarter END timestamp\n",
    "compq['date'] = pd.PeriodIndex(compq['datadate'], freq='Q').to_timestamp(how='end')\n",
    "compq['date'] = compq['date'].dt.to_period('M').dt.to_timestamp('M')  # normalize to month-end\n",
    "\n",
    "# Build features (protect against divide-by-zero with where/clip)\n",
    "eps = 1e-12\n",
    "\n",
    "# 1) Size (log of assets)\n",
    "compq['size_log_at'] = np.log(compq['atq'].where(compq['atq'] > 0))\n",
    "\n",
    "# 2) Book-to-Market (quarterly proxy): common equity / assets\n",
    "compq['bm'] = (compq['ceqq'] / compq['atq']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 3) Leverage: (long-term + short-term debt) / assets\n",
    "compq['lev'] = ((compq['dlttq'].fillna(0) + compq['dlcq'].fillna(0)) / compq['atq']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 4) Sales growth QoQ: (revtq / L1(revtq) - 1)\n",
    "compq['revtq_l1'] = compq.groupby('gvkey')['revtq'].shift(1)\n",
    "compq['sales_g_qoq'] = (compq['revtq'] / compq['revtq_l1'] - 1).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 5) Sales growth YoY: (revtq / L4(revtq) - 1)\n",
    "compq['revtq_l4'] = compq.groupby('gvkey')['revtq'].shift(4)\n",
    "compq['sales_g_yoy'] = (compq['revtq'] / compq['revtq_l4'] - 1).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 6) Asset growth YoY: (atq / L4(atq) - 1)\n",
    "compq['atq_l4'] = compq.groupby('gvkey')['atq'].shift(4)\n",
    "compq['asset_g_yoy'] = (compq['atq'] / compq['atq_l4'] - 1).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 7) Profitability (ROA): ibq / assets (can also use lagged assets; here use contemporaneous as a simple proxy)\n",
    "compq['roa'] = (compq['ibq'] / compq['atq']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 8) Gross margin: (revtq - cogsq) / revtq\n",
    "compq['gross_margin'] = ((compq['revtq'] - compq['cogsq']) / compq['revtq']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 9) Capex-to-assets: capxq / atq\n",
    "compq['capex_to_assets'] = (compq['capxq'] / compq['atq']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 10) Cash-to-assets: cheq / atq\n",
    "compq['cash_to_assets'] = (compq['cheq'] / compq['atq']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Keep only what we need going forward\n",
    "compq_features = compq[[\n",
    "    'gvkey', 'datadate', 'date',\n",
    "    'size_log_at', 'bm', 'lev',\n",
    "    'sales_g_qoq', 'sales_g_yoy', 'asset_g_yoy',\n",
    "    'roa', 'gross_margin', 'capex_to_assets', 'cash_to_assets'\n",
    "]].copy()\n",
    "\n",
    "# (Optional) De-duplicate quarterly rows per gvkey-datadate if needed\n",
    "compq_features = compq_features.drop_duplicates(subset=['gvkey', 'datadate'])\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "127aeb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 4. Merge Data (Permno / GVKEY link needed)\n",
    "# ---------------------------\n",
    "# Note: requires CRSP-Compustat linking table (ccmxpf_linktable)\n",
    "link_table = pd.read_sas(os.path.join('data', 'l4', 'ccmxpf_linktable.sas7bdat'), format='sas7bdat', encoding='utf-8').rename(\n",
    "    columns={'lpermno': 'permno'})\n",
    "\n",
    "link_table['linkdt'] = pd.to_datetime(link_table['linkdt'])\n",
    "link_table['linkenddt'] = pd.to_datetime(link_table['linkenddt'])\n",
    "\n",
    "# Keep common, high-quality link types and primaries\n",
    "link_keep_types = {'LC', 'LU'}   # standard/unique CRSP-permno â†” Compustat-gvkey links\n",
    "link_keep_prim  = {'P', 'C'}     # prefer primary (P), then consolidated (C)\n",
    "\n",
    "link_table = link_table.loc[\n",
    "    link_table['linktype'].isin(link_keep_types) &\n",
    "    link_table['linkprim'].isin(link_keep_prim),\n",
    "    ['gvkey', 'permno', 'linkdt', 'linkenddt', 'linktype', 'linkprim']\n",
    "].copy()\n",
    "\n",
    "# Treat open-ended linkenddt as far-future to simplify interval filtering\n",
    "link_table['linkenddt'] = link_table['linkenddt'].fillna(pd.Timestamp('2099-12-31'))\n",
    "\n",
    "# 4.1 First, join CRSP to link table on permno, then keep only rows where the CRSP month lies within the valid link interval\n",
    "crsp_lnk = crsp_msf.merge(link_table, on='permno', how='left')\n",
    "mask_valid_interval = (crsp_lnk['date'] >= crsp_lnk['linkdt']) & (crsp_lnk['date'] <= crsp_lnk['linkenddt'])\n",
    "crsp_lnk = crsp_lnk.loc[mask_valid_interval].copy()\n",
    "\n",
    "# If multiple gvkeys match the same (permno, date), keep the \"best\" one:\n",
    "#   - Prefer linkprim=P over C; prefer linktype=LC over LU; if tied, keep the most recent linkdt\n",
    "prim_order = {'P': 0, 'C': 1}\n",
    "type_order = {'LC': 0, 'LU': 1}\n",
    "crsp_lnk['prim_rank'] = crsp_lnk['linkprim'].map(prim_order)\n",
    "crsp_lnk['type_rank'] = crsp_lnk['linktype'].map(type_order)\n",
    "\n",
    "crsp_lnk = (crsp_lnk\n",
    "    .sort_values(['permno', 'date', 'prim_rank', 'type_rank', 'linkdt'],\n",
    "                 ascending=[True, True, True, True, False])\n",
    "    .drop_duplicates(subset=['permno', 'date'], keep='first')\n",
    "    .drop(columns=['prim_rank', 'type_rank'])\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a85de018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final merged shape: (718001, 30)\n",
      "   permno   gvkey       date    datem   dateq       ret   mktrf     smb  \\\n",
      "0   10001  012994 2010-01-29  2010-01  2010Q1 -0.018932 -0.0335  0.0043   \n",
      "1   10001  012994 2010-02-26  2010-02  2010Q1 -0.000656  0.0339  0.0118   \n",
      "2   10001  012994 2010-03-31  2010-03  2010Q1  0.020643   0.063  0.0146   \n",
      "3   10001  012994 2010-04-30  2010-04  2010Q2  0.124385  0.0199  0.0484   \n",
      "4   10001  012994 2010-05-28  2010-05  2010Q2  0.004829  -0.079  0.0013   \n",
      "\n",
      "      hml      rf  size_log_at        bm       lev  \n",
      "0  0.0033     0.0     4.771168  0.468918  0.350051  \n",
      "1  0.0318     0.0     4.771168  0.468918  0.350051  \n",
      "2  0.0219  0.0001     4.771168  0.468918  0.350051  \n",
      "3  0.0296  0.0001     4.770549  0.467106  0.362329  \n",
      "4 -0.0248  0.0001     4.770549  0.467106  0.362329  \n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Build month and quarter keys; merge CRSP + FF (by month) + CompustatQ (by quarter)\n",
    "# ==========================================\n",
    "\n",
    "# --- 0) Normalize dates to pandas Timestamps (if not already) ---\n",
    "crsp_lnk['date'] = pd.to_datetime(crsp_lnk['date'], errors='coerce')\n",
    "ff_factors['date'] = pd.to_datetime(ff_factors['date'], errors='coerce')\n",
    "compq_features['datadate'] = pd.to_datetime(compq_features['datadate'], errors='coerce')\n",
    "compq_features['date'] = pd.to_datetime(compq_features['date'], errors='coerce')  # quarter end already\n",
    "\n",
    "# --- 1) Create merge keys ---\n",
    "# CRSP: month and quarter from the CRSP month-end date\n",
    "crsp_lnk['datem'] = crsp_lnk['date'].dt.to_period('M')  # YYYY-MM\n",
    "crsp_lnk['dateq'] = crsp_lnk['date'].dt.to_period('Q')  # YYYYQn\n",
    "\n",
    "# FF factors: merge on month (datem)\n",
    "ff_factors['datem'] = ff_factors['date'].dt.to_period('M')  # YYYY-MM\n",
    "\n",
    "# CompustatQ: create quarter key from the financial report date (prefer datadateâ€™s fiscal quarter)\n",
    "# Using datadate ensures we do NOT leak future information before the filing quarter.\n",
    "compq_features['dateq'] = compq_features['datadate'].dt.to_period('Q')  # YYYYQn\n",
    "\n",
    "# Optional safety: ensure at most one Compustat row per (gvkey, dateq)\n",
    "compq_q = (compq_features\n",
    "           .sort_values(['gvkey', 'datadate'])\n",
    "           .drop_duplicates(subset=['gvkey', 'dateq']))\n",
    "\n",
    "# --- 2) Merge CRSP with Famaâ€“French by month key (datem) ---\n",
    "crsp_ff = crsp_lnk.merge(\n",
    "    ff_factors[['datem', 'mktrf', 'smb', 'hml', 'rf']],\n",
    "    on='datem',\n",
    "    how='left',\n",
    "    validate='many_to_one'  # one factor vector per month\n",
    ")\n",
    "\n",
    "# --- 3) Merge CRSP(+FF) with CompustatQ by quarter key (dateq) and gvkey ---\n",
    "# Requires crsp_lnk to already have gvkey via the CCM link step\n",
    "final_data = crsp_ff.merge(\n",
    "    compq_q[['gvkey', 'dateq',\n",
    "             'size_log_at', 'bm', 'lev',\n",
    "             'sales_g_qoq', 'sales_g_yoy', 'asset_g_yoy',\n",
    "             'roa', 'gross_margin', 'capex_to_assets', 'cash_to_assets']],\n",
    "    on=['gvkey', 'dateq'],\n",
    "    how='left',\n",
    "    validate='many_to_one'  # one accounting record per gvkey per quarter\n",
    ")\n",
    "\n",
    "# --- 4) Optional: basic cleanup ---\n",
    "# Drop exact duplicates on (permno, date); keep first occurrence\n",
    "final_data = final_data.drop_duplicates(subset=['permno', 'date']).copy()\n",
    "\n",
    "# --- 5) Inspect result ---\n",
    "print(\"Final merged shape:\", final_data.shape)\n",
    "print(final_data[['permno','gvkey','date','datem','dateq',\n",
    "                  'ret','mktrf','smb','hml','rf',\n",
    "                  'size_log_at','bm','lev']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e63c029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 313,729 | Test rows: 123,655\n",
      "Train period: 2010-12-31 ~ 2017-12-29\n",
      "Test  period: 2018-01-31 ~ 2020-11-30\n",
      "Train: 2010-12-31 00:00:00 ~ 2017-12-29 00:00:00 313729\n",
      "Test : 2018-01-31 00:00:00 ~ 2020-11-30 00:00:00 123655\n"
     ]
    }
   ],
   "source": [
    "# === Preprocessing (monthly, out-of-sample safe) ===\n",
    "# ---------------------------\n",
    "# 0) Basic hygiene\n",
    "# ---------------------------\n",
    "df = final_data.copy()\n",
    "\n",
    "# Ensure date sorted\n",
    "df = df.sort_values(['permno', 'date'])\n",
    "\n",
    "acct_cols = ['size_log_at','bm','lev','sales_g_qoq','sales_g_yoy','asset_g_yoy',\n",
    "             'roa','gross_margin','capex_to_assets','cash_to_assets']\n",
    "for key in ['bm', 'lev', 'sales_g_qoq', 'sales_g_yoy', 'asset_g_yoy',\n",
    "            'roa', 'gross_margin', 'capex_to_assets', 'cash_to_assets']:\n",
    "    df[key] = df[key].astype(np.float64)\n",
    "\n",
    "# Keep core columns presence\n",
    "needed_cols = ['permno','date','ret','rf','me','mktrf','smb','hml'] + acct_cols\n",
    "\n",
    "for c in needed_cols:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f\"Missing required column: {c}. Please check data loading step.\")\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Target: next-month excess return\n",
    "# ---------------------------\n",
    "df['excess_ret'] = df['ret'] - df['rf']  # current month excess return\n",
    "df['excess_ret_fwd1'] = df.groupby('permno')['excess_ret'].shift(-1)  # predict next month\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Feature engineering (all lagged to avoid look-ahead)\n",
    "#    - Size (log ME) already as 'size' from Compustat; keep a CRSP-based log(ME) too for robustness\n",
    "#    - Book-to-Market 'bm' from Compustat\n",
    "#    - Momentum 12-2 (skip most recent month): sum of returns t-12..t-2\n",
    "#    - Volatility 6m: rolling std of monthly returns\n",
    "#    - Past market factor (lagged): mktrf_{t-1}\n",
    "# ---------------------------\n",
    "\n",
    "# CRSP-based log(ME)\n",
    "df['log_me'] = np.log(df['me'].replace({0: np.nan}))\n",
    "\n",
    "# Momentum 12-2:\n",
    "# comp_12 = (1+ret).rolling(12).prod() - 1\n",
    "# then subtract last month's return (t-1) to \"skip\" the most recent month\n",
    "comp_12 = df.groupby('permno')['ret'].transform(\n",
    "    lambda x: (1 + x).rolling(12).apply(np.prod, raw=True) - 1\n",
    ")\n",
    "last_m = df.groupby('permno', sort=False)['ret'].shift(1)\n",
    "df['mom_12_2'] = comp_12 - last_m\n",
    "\n",
    "# Volatility 6m (rolling std over last 6 months)\n",
    "df['vol_6m'] = df.groupby('permno', sort=False)['ret'] \\\n",
    "                 .transform(lambda x: x.rolling(6).std())\n",
    "\n",
    "# Lagged market factor (last month, global series)\n",
    "df = df.sort_values(['date'])  # ensure chronological order for global lag\n",
    "df['mktrf_lag1'] = df['mktrf'].shift(1)\n",
    "df['smb_lag1']   = df['smb'].shift(1)\n",
    "df['hml_lag1']   = df['hml'].shift(1)\n",
    "df['rf_lag1']    = df['rf'].shift(1)\n",
    "\n",
    "# Make sure fundamentals are carried forward within each stock (or use gvkey if preferred)\n",
    "df = df.sort_values(['permno', 'date'])\n",
    "df[['size_log_at', 'bm']] = df.groupby('permno')[['size_log_at', 'bm']].ffill()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Assemble feature set\n",
    "# ---------------------------\n",
    "feature_cols = [\n",
    "    # CRSP-derived\n",
    "    'log_me', 'mom_12_2', 'vol_6m',\n",
    "    # FF factors\n",
    "    'mktrf', 'smb', 'hml', 'rf',\n",
    "    # Compustat quarterly fundamentals (carried forward)\n",
    "    'size_log_at', 'bm', 'lev', 'sales_g_qoq', 'sales_g_yoy',\n",
    "    'asset_g_yoy', 'roa', 'gross_margin', 'capex_to_assets', 'cash_to_assets'\n",
    "]\n",
    "\n",
    "# Drop rows where core target or features missing\n",
    "cols_needed_for_model = ['permno','date','excess_ret_fwd1'] + feature_cols\n",
    "df_model = df[cols_needed_for_model].replace([np.inf, -np.inf], np.nan).dropna(how='any')\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Winsorization (1% / 99%) on features to reduce tail sensitivity\n",
    "# ---------------------------\n",
    "def winsorize_df(dfin, cols, p=0.01):\n",
    "    d = dfin.copy()\n",
    "    for c in cols:\n",
    "        q_low, q_high = d[c].quantile([p, 1-p])\n",
    "        d[c] = d[c].clip(lower=q_low, upper=q_high)\n",
    "    return d\n",
    "\n",
    "df_model = winsorize_df(df_model, feature_cols, p=0.01)\n",
    "df_model['target'] = df_model['excess_ret_fwd1']\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Temporal train/test split (expandable; here: fixed split)\n",
    "#    Train: 2005-01-01 ~ 2017-12-31\n",
    "#    Test : 2018-01-01 ~ 2024-12-31\n",
    "# ---------------------------\n",
    "train_end = pd.Timestamp('2017-12-31')\n",
    "test_start = pd.Timestamp('2018-01-01')\n",
    "\n",
    "train_mask = df_model['date'] <= train_end\n",
    "test_mask  = df_model['date'] >= test_start\n",
    "\n",
    "train_df = df_model.loc[train_mask].dropna(how='any')\n",
    "test_df  = df_model.loc[test_mask].dropna(how='any')\n",
    "\n",
    "# # Shuffle is NOT used (time-series); keep order if needed for rolling evaluation.\n",
    "# # Basic checks\n",
    "# print(f\"Train rows: {len(train_df):,} | Test rows: {len(test_df):,}\")\n",
    "# print(f\"Train period: {train_df['date'].min().date()} ~ {train_df['date'].max().date()}\")\n",
    "# print(f\"Test  period: {test_df['date'].min().date()} ~ {test_df['date'].max().date()}\")\n",
    "\n",
    "# X_train = train_df[feature_cols].to_numpy(dtype='float64')\n",
    "# y_train = train_df['target'].to_numpy(dtype='float64')\n",
    "# X_test  = test_df[feature_cols].to_numpy(dtype='float64')\n",
    "# y_test  = test_df['target'].to_numpy(dtype='float64')\n",
    "\n",
    "# print(\"Train:\", train_df['date'].min(), \"~\", train_df['date'].max(), len(train_df))\n",
    "# print(\"Test :\", test_df['date'].min(), \"~\", test_df['date'].max(), len(test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1b8e9963",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_model[feature_cols].to_numpy(dtype='float64')\n",
    "y = df_model['excess_ret_fwd1'].to_numpy(dtype='float64')\n",
    "\n",
    "# 2) Standardize features (recommended for linear models)\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "942c9987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "try:\n",
    "    from scipy.stats import norm\n",
    "    _HAS_SCIPY = True\n",
    "except Exception:\n",
    "    _HAS_SCIPY = False\n",
    "    \n",
    "def _nw_var(d, lag=0):\n",
    "    \"\"\"\n",
    "    Neweyâ€“West variance of the mean of d (loss differential), lag = q.\n",
    "    Returns var(mean(d)) = (gamma0 + 2*sum w_k*gamma_k)/T\n",
    "    \"\"\"\n",
    "    d = np.asarray(d, float)\n",
    "    T = len(d)\n",
    "    d = d - d.mean()\n",
    "    gamma0 = np.dot(d, d) / T\n",
    "    var = gamma0\n",
    "    for k in range(1, lag + 1):\n",
    "        w = 1.0 - k / (lag + 1.0)  # Bartlett weights\n",
    "        cov = np.dot(d[k:], d[:-k]) / T\n",
    "        var += 2.0 * w * cov\n",
    "    return var / T  # variance of the mean\n",
    "\n",
    "def dm_test(y_true, y_model, y_bench, lag=0):\n",
    "    \"\"\"\n",
    "    Dieboldâ€“Mariano test (squared-error loss) comparing model vs benchmark.\n",
    "    H0: E(loss_model - loss_bench) = 0  (no improvement)\n",
    "    Returns: (stat, p_two_sided, p_one_sided) where one-sided favors the model.\n",
    "    \"\"\"\n",
    "    e_m = y_true - y_model\n",
    "    e_b = y_true - y_bench\n",
    "    d = (e_b**2) - (e_m**2)  # positive => model improves over benchmark\n",
    "    m = np.mean(d)\n",
    "    v = _nw_var(d, lag=lag)\n",
    "    stat = m / sqrt(v) if v > 0 else np.nan\n",
    "    if _HAS_SCIPY:\n",
    "        p_two = 2.0 * (1.0 - norm.cdf(abs(stat)))\n",
    "        p_one = 1.0 - norm.cdf(stat)  # H1: model better (stat > 0)\n",
    "    else:\n",
    "        # simple normal approx via error function\n",
    "        from math import erf\n",
    "        def _phi(z): return 0.5 * (1.0 + erf(z / np.sqrt(2.0)))\n",
    "        p_two = 2.0 * (1.0 - _phi(abs(stat)))\n",
    "        p_one = 1.0 - _phi(stat)\n",
    "    return stat, p_two, p_one\n",
    "\n",
    "def clark_west_test(y_true, f_model, f_bench, lag=0):\n",
    "    \"\"\"\n",
    "    Clarkâ€“West (2007) test for nested models vs benchmark (squared-error loss).\n",
    "    Let e_m = y - f_model, e_b = y - f_bench.\n",
    "    Adjusted differential: d_t = e_b^2 - ( e_m^2 - (f_b - f_m)^2 )\n",
    "    H0: E(d_t) = 0 (no improvement); H1: E(d_t) > 0 (model improves).\n",
    "    Returns: (stat, p_one_sided)\n",
    "    \"\"\"\n",
    "    e_m = y_true - f_model\n",
    "    e_b = y_true - f_bench\n",
    "    d = (e_b**2) - (e_m**2 - (f_bench - f_model)**2)\n",
    "    m = np.mean(d)\n",
    "    v = _nw_var(d, lag=lag)\n",
    "    stat = m / sqrt(v) if v > 0 else np.nan\n",
    "    if _HAS_SCIPY:\n",
    "        p_one = 1.0 - norm.cdf(stat)\n",
    "    else:\n",
    "        from math import erf\n",
    "        def _phi(z): return 0.5 * (1.0 + erf(z / np.sqrt(2.0)))\n",
    "        p_one = 1.0 - _phi(stat)\n",
    "    return stat, p_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "973bbd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "def evaluate_model_rolling(\n",
    "    model,\n",
    "    X,\n",
    "    y,\n",
    "    dates=None,\n",
    "    train_window=120,     # number of observations in the training window (for rolling)\n",
    "    test_horizon=1,       # predict the next H observations each step (H=1 is most common)\n",
    "    step=1,               # how many observations to move the window forward each iteration\n",
    "    scaler=None,          # optional sklearn transformer, e.g., StandardScaler()\n",
    "    model_name=None,\n",
    "    hac_lag=0,            # Neweyâ€“West lag for DM/CW tests (0 for non-overlapping horizons)\n",
    "    expanding=False,      # True = expanding window; False = fixed-length rolling window\n",
    "    dropna=True           # drop windows with NaNs in train/test\n",
    "):\n",
    "    \"\"\"\n",
    "    Rolling / Expanding out-of-sample evaluation for a predictive model.\n",
    "\n",
    "    Workflow per step:\n",
    "      1) Fit the model on the training window (rolling or expanding).\n",
    "      2) Predict the next `test_horizon` observations.\n",
    "      3) Build a benchmark using the training-window historical mean.\n",
    "      4) Accumulate predictions/targets across all steps.\n",
    "      5) Compute performance metrics and tests on the concatenated OOS sample.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : sklearn-compatible estimator\n",
    "        Must implement fit(X, y) and predict(X).\n",
    "    X : array-like or DataFrame, shape (T, K)\n",
    "        Features sorted in chronological order by rows (oldest to newest).\n",
    "    y : array-like, shape (T,)\n",
    "        Target aligned to X (y[t] corresponds to X[t, :]).\n",
    "    dates : array-like or DatetimeIndex, optional\n",
    "        Timestamps for rows of X/y (used in the detailed return).\n",
    "        If None and X is a DataFrame with a DatetimeIndex, that index will be used.\n",
    "    train_window : int\n",
    "        Size of training window for rolling (ignored if expanding=True).\n",
    "    test_horizon : int\n",
    "        Number of future observations to predict per step (typical choice is 1).\n",
    "    step : int\n",
    "        Window shift size between iterations.\n",
    "    scaler : sklearn transformer, optional\n",
    "        If provided, it will be fit on each training window and applied to both train/test.\n",
    "    model_name : str, optional\n",
    "        Label for the output table. Defaults to estimator class name.\n",
    "    hac_lag : int\n",
    "        Neweyâ€“West lag for DM and Clarkâ€“West tests (set >0 if horizons overlap).\n",
    "    expanding : bool\n",
    "        If True, uses expanding window; otherwise uses fixed-size rolling window.\n",
    "    dropna : bool\n",
    "        If True, drops NaNs inside each train/test window before fitting/evaluating.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : pd.DataFrame\n",
    "        Single-row summary with RMSE, MAE, HitRatio, R2_OOS, DM_stat, CW_stat,\n",
    "        number of fits, total/mean fit time.\n",
    "    detailed : pd.DataFrame\n",
    "        Row-by-row OOS records with columns: ['date', 'y', 'y_hat', 'y_bench'].\n",
    "    \"\"\"\n",
    "\n",
    "    # ---- Convert inputs to numpy arrays; capture dates ----\n",
    "    if isinstance(X, (pd.DataFrame, pd.Series)):\n",
    "        X_arr = X.values\n",
    "    else:\n",
    "        X_arr = np.asarray(X)\n",
    "\n",
    "    y_arr = np.asarray(y).reshape(-1,)\n",
    "    T = len(y_arr)\n",
    "\n",
    "    if dates is not None:\n",
    "        dates_arr = np.asarray(dates)\n",
    "    elif isinstance(X, pd.DataFrame) and isinstance(X.index, pd.DatetimeIndex):\n",
    "        dates_arr = X.index.to_numpy()\n",
    "    else:\n",
    "        dates_arr = np.arange(T)  # fallback index\n",
    "\n",
    "    # ---- Containers for OOS predictions and bookkeeping ----\n",
    "    preds_all, y_all, bench_all, ts_all = [], [], [], []\n",
    "    fit_times = []\n",
    "    n_fits = 0\n",
    "\n",
    "    # Starting index for the first evaluation step\n",
    "    # For expanding: start with at least 1 obs; for rolling: need `train_window` obs\n",
    "    start_idx = 1 if expanding else train_window\n",
    "\n",
    "    # Loop over windows\n",
    "    for i in range(start_idx, T - test_horizon + 1, step):\n",
    "        # Training slice\n",
    "        tr_start = 0 if expanding else i - train_window\n",
    "        tr_end   = i                  # train on [tr_start, tr_end)\n",
    "        te_start = i\n",
    "        te_end   = i + test_horizon   # test on [te_start, te_end)\n",
    "\n",
    "        X_tr, y_tr = X_arr[tr_start:tr_end, :], y_arr[tr_start:tr_end]\n",
    "        X_te, y_te = X_arr[te_start:te_end,   :], y_arr[te_start:te_end]\n",
    "        te_dates   = dates_arr[te_start:te_end]\n",
    "\n",
    "        # Optional NaN handling within each window\n",
    "        if dropna:\n",
    "            tr_mask = np.isfinite(y_tr)\n",
    "            if tr_mask.sum() < 3:\n",
    "                continue\n",
    "            X_tr, y_tr = X_tr[tr_mask, :], y_tr[tr_mask]\n",
    "\n",
    "            te_mask = np.isfinite(y_te)\n",
    "            if te_mask.sum() == 0:\n",
    "                continue\n",
    "            X_te, y_te = X_te[te_mask, :], y_te[te_mask]\n",
    "            te_dates   = te_dates[te_mask]\n",
    "\n",
    "        # Optional scaling: fit scaler on the training window only\n",
    "        if scaler is not None:\n",
    "            sc = clone(scaler)\n",
    "            X_tr_s = sc.fit_transform(X_tr)\n",
    "            X_te_s = sc.transform(X_te)\n",
    "        else:\n",
    "            X_tr_s, X_te_s = X_tr, X_te\n",
    "\n",
    "        # Clone model to ensure independent refit each step\n",
    "        mdl = clone(model)\n",
    "\n",
    "        # Fit and time it\n",
    "        t0 = time.time()\n",
    "        mdl.fit(X_tr_s, y_tr)\n",
    "        fit_times.append(time.time() - t0)\n",
    "        n_fits += 1\n",
    "\n",
    "        # Predict on the test slice\n",
    "        y_hat = np.asarray(mdl.predict(X_te_s)).reshape(-1,)\n",
    "\n",
    "        # Benchmark: historical mean from the *training* window\n",
    "        mu_hat = float(np.nanmean(y_tr))\n",
    "        y_bmk  = np.full_like(y_te, mu_hat, dtype=float)\n",
    "\n",
    "        # Accumulate\n",
    "        preds_all.append(y_hat)\n",
    "        y_all.append(y_te)\n",
    "        bench_all.append(y_bmk)\n",
    "        ts_all.append(te_dates)\n",
    "\n",
    "    # Concatenate across all steps\n",
    "    if len(y_all) == 0:\n",
    "        raise ValueError(\"No test observations produced. Check window sizes or data length.\")\n",
    "\n",
    "    y_pred  = np.concatenate(preds_all)\n",
    "    y_true  = np.concatenate(y_all)\n",
    "    y_bench = np.concatenate(bench_all)\n",
    "    t_out   = np.concatenate(ts_all)\n",
    "\n",
    "    # ---- Metrics on the concatenated OOS sample ----\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    mae  = float(mean_absolute_error(y_true, y_pred))\n",
    "    hit  = float((np.sign(y_pred) == np.sign(y_true)).mean())\n",
    "\n",
    "    sse_model = float(np.sum((y_true  - y_pred)  ** 2))\n",
    "    sse_bench = float(np.sum((y_true  - y_bench) ** 2))\n",
    "    r2_oos = 1.0 - sse_model / sse_bench if sse_bench > 0 else np.nan\n",
    "\n",
    "    # DM (squared-error loss) and Clarkâ€“West (nested models) vs benchmark\n",
    "    dm_stat, dm_p_two, dm_p_one = dm_test(y_true, y_pred, y_bench, lag=hac_lag)\n",
    "    cw_stat, cw_p_one = clark_west_test(y_true, y_pred, y_bench, lag=hac_lag)\n",
    "\n",
    "    # Fit-time accounting\n",
    "    fit_time_total = float(np.sum(fit_times)) if fit_times else 0.0\n",
    "    fit_time_mean  = float(np.mean(fit_times)) if fit_times else 0.0\n",
    "\n",
    "    # One-row summary for pretty printing\n",
    "    results = pd.DataFrame([{\n",
    "        \"Model\": model_name or model.__class__.__name__,\n",
    "        \"RMSE\": round(rmse, 6),\n",
    "        \"MAE\": round(mae, 6),\n",
    "        \"HitRatio\": round(hit, 4),\n",
    "        \"R2_OOS\": round(r2_oos, 6),\n",
    "        \"DM_stat\": round(dm_stat, 3),\n",
    "        \"CW_stat\": round(cw_stat, 3),\n",
    "        \"Fits\": int(n_fits),\n",
    "        \"FitTime_Total (s)\": round(fit_time_total, 4),\n",
    "        \"FitTime_Mean (s)\": round(fit_time_mean, 4),\n",
    "    }])\n",
    "\n",
    "    # Row-level OOS records for plotting and segment analysis\n",
    "    detailed = pd.DataFrame({\n",
    "        \"date\": t_out,\n",
    "        \"y\": y_true,\n",
    "        \"y_hat\": y_pred,\n",
    "        \"y_bench\": y_bench\n",
    "    }).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    return results, detailed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915eec63",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares (OLS)\n",
    "\n",
    "**Idea:**  \n",
    "Ordinary Least Squares (OLS) is the baseline linear regression method. It estimates the relationship between a dependent variable $y$ and explanatory variables $X$ by fitting a straight line (or hyperplane) that minimizes the **sum of squared errors**.\n",
    "\n",
    "**Model:**\n",
    "$$\n",
    "y_i = \\alpha + X_i^\\top \\beta + \\varepsilon_i\n",
    "$$\n",
    "\n",
    "- $y_i$: outcome (e.g., next-month excess return)  \n",
    "- $X_i$: vector of predictors (firm characteristics, factors)  \n",
    "- $\\beta$: coefficients to be estimated  \n",
    "- $\\varepsilon_i$: error term (unexplained part)\n",
    "\n",
    "**Estimation Principle:**  \n",
    "OLS chooses $\\hat{\\beta}$ to minimize\n",
    "$$\n",
    "\\min_{\\beta} \\sum_{i=1}^n \\left(y_i - X_i^\\top \\beta \\right)^2\n",
    "$$\n",
    "\n",
    "**Interpretation:**  \n",
    "- Each $\\beta_j$ shows how a one-unit change in predictor $X_j$ affects the expected outcome, holding others constant.  \n",
    "- Provides a simple and interpretable baseline.  \n",
    "\n",
    "**Limitations:**  \n",
    "- Assumes linear relationships.  \n",
    "- Sensitive to multicollinearity and outliers.  \n",
    "- May overfit when predictors are high-dimensional and noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "35975728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OLS Test Results ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>HitRatio</th>\n",
       "      <th>FitTime (s)</th>\n",
       "      <th>R2_OOS</th>\n",
       "      <th>DM_stat</th>\n",
       "      <th>DM_p(two)</th>\n",
       "      <th>DM_p(one)</th>\n",
       "      <th>CW_stat</th>\n",
       "      <th>CW_p(one)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OLS</td>\n",
       "      <td>0.047272</td>\n",
       "      <td>0.03595</td>\n",
       "      <td>0.7021</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.020472</td>\n",
       "      <td>-0.758</td>\n",
       "      <td>0.4486</td>\n",
       "      <td>0.7757</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.4441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model      RMSE      MAE  HitRatio  FitTime (s)    R2_OOS  DM_stat  \\\n",
       "0   OLS  0.047272  0.03595    0.7021        0.002 -0.020472   -0.758   \n",
       "\n",
       "   DM_p(two)  DM_p(one)  CW_stat  CW_p(one)  \n",
       "0     0.4486     0.7757    0.141     0.4441  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1) Build matrices\n",
    "# X_train = train_df[feature_cols].to_numpy(dtype='float64')\n",
    "# y_train = train_df['excess_ret_fwd1'].to_numpy(dtype='float64')\n",
    "# X_test  = test_df[feature_cols].to_numpy(dtype='float64')\n",
    "# y_test  = test_df['excess_ret_fwd1'].to_numpy(dtype='float64')\n",
    "\n",
    "# 2) Standardize features (recommended for linear models)\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std  = scaler.transform(X_test)\n",
    "\n",
    "# 3) Fit and evaluate OLS\n",
    "ols = LinearRegression()\n",
    "ols_results, ols_model = evaluate_model(ols, X_train_std, y_train, X_test_std, y_test,\n",
    "                            model_name=\"OLS\", hac_lag=0)\n",
    "\n",
    "# 4) Print evaluation summary\n",
    "print(\"=== OLS Test Results ===\")\n",
    "display(ols_results)\n",
    "\n",
    "all_results = []\n",
    "all_results.append(ols_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45377a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7cef5ff",
   "metadata": {},
   "source": [
    "## Regularized Regression: Ridge, Lasso, and Elastic Net\n",
    "\n",
    "**Motivation:**  \n",
    "Ordinary Least Squares (OLS) can overfit when predictors are **high-dimensional**, **noisy**, or **highly correlated**.  \n",
    "Regularization methods add a penalty to the loss function, shrinking coefficients to improve generalization.\n",
    "\n",
    "---\n",
    "\n",
    "### Lasso Regression (L1 penalty)\n",
    "Lasso uses the **sum of absolute coefficients** as a penalty:\n",
    "$$\n",
    "\\min_{\\beta} \\sum_{i=1}^n (y_i - X_i^\\top \\beta)^2 + \\lambda \\sum_{j=1}^p |\\beta_j|\n",
    "$$\n",
    "\n",
    "- Performs **variable selection**: some coefficients shrink exactly to zero.  \n",
    "- Useful when we believe only a subset of predictors are truly important.  \n",
    "- Tends to select one variable from a group of correlated variables.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Ridge Regression (L2 penalty)\n",
    "Ridge shrinks coefficients by adding the **sum of squared coefficients** as a penalty:\n",
    "$$\n",
    "\\min_{\\beta} \\sum_{i=1}^n (y_i - X_i^\\top \\beta)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\n",
    "$$\n",
    "\n",
    "- Keeps all predictors, but shrinks them toward zero.  \n",
    "- Works well when predictors are correlated.  \n",
    "- Tuning parameter $\\lambda$ controls the amount of shrinkage.\n",
    "---\n",
    "\n",
    "### Elastic Net (L1 + L2 penalty)\n",
    "Elastic Net combines both L1 and L2 penalties:\n",
    "$$\n",
    "\\min_{\\beta} \\sum_{i=1}^n (y_i - X_i^\\top \\beta)^2 + \\lambda_1 \\sum_{j=1}^p |\\beta_j| + \\lambda_2 \\sum_{j=1}^p \\beta_j^2\n",
    "$$\n",
    "\n",
    "- Balances **shrinkage** (Ridge) and **variable selection** (Lasso).  \n",
    "- Works better than Lasso when predictors are highly correlated.  \n",
    "- Controlled by two hyperparameters: $\\lambda_1$ (L1 strength) and $\\lambda_2$ (L2 strength).\n",
    "\n",
    "---\n",
    "\n",
    "**Takeaway:**  \n",
    "- **Lasso** â†’ performs automatic feature selection.  \n",
    "- **Ridge** â†’ keeps all predictors, reduces variance.  \n",
    "- **Elastic Net** â†’ compromise, often best in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "02a445a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LassoCV] best alpha = 10\n",
      "=== Lasso Test Results ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>HitRatio</th>\n",
       "      <th>FitTime (s)</th>\n",
       "      <th>R2_OOS</th>\n",
       "      <th>DM_stat</th>\n",
       "      <th>DM_p(two)</th>\n",
       "      <th>DM_p(one)</th>\n",
       "      <th>CW_stat</th>\n",
       "      <th>CW_p(one)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lasso(alpha=10)</td>\n",
       "      <td>0.046795</td>\n",
       "      <td>0.036413</td>\n",
       "      <td>0.7128</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model      RMSE       MAE  HitRatio  FitTime (s)  R2_OOS  \\\n",
       "0  Lasso(alpha=10)  0.046795  0.036413    0.7128        0.001     0.0   \n",
       "\n",
       "   DM_stat  DM_p(two)  DM_p(one)  CW_stat  CW_p(one)  \n",
       "0      NaN        NaN        NaN      NaN        NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Lasso Regression ===\n",
    "# 1) Choose alpha via CV (no shuffle to respect time ordering)\n",
    "alphas = np.logspace(-4, 1, 40)  # 1e-4 â€¦ 10\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=5, fit_intercept=True, max_iter=5000, n_jobs=-1, random_state=42)\n",
    "lasso_cv.fit(X_train_std, y_train)\n",
    "best_alpha = float(lasso_cv.alpha_)\n",
    "print(f\"[LassoCV] best alpha = {best_alpha:.6g}\")\n",
    "\n",
    "# 2) Refit Lasso with best alpha (so fit time is comparable to OLS)\n",
    "lasso = Lasso(alpha=best_alpha, fit_intercept=True, max_iter=5000, random_state=42)\n",
    "lasso_results, lasso_model = evaluate_model(\n",
    "    lasso, X_train_std, y_train, X_test_std, y_test, model_name=f\"Lasso(alpha={best_alpha:.3g})\", hac_lag=0\n",
    ")\n",
    "\n",
    "print(\"=== Lasso Test Results ===\")\n",
    "display(lasso_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ae91443b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LassoCV] best alpha = 0.0001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[172]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m lasso = Lasso(alpha=best_alpha, fit_intercept=\u001b[38;5;28;01mTrue\u001b[39;00m, max_iter=\u001b[32m5000\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# 3) Rolling evaluation (example: 60-month rolling window, 12-month test window)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m lasso_results, lasso_details = evaluate_model_rolling(\n\u001b[32m     22\u001b[39m     model=lasso,\n\u001b[32m     23\u001b[39m     X=X_std,              \u001b[38;5;66;03m# full standardized features\u001b[39;00m\n\u001b[32m     24\u001b[39m     y=y,                  \u001b[38;5;66;03m# full target returns\u001b[39;00m\n\u001b[32m     25\u001b[39m     train_window=\u001b[32m60\u001b[39m,       \u001b[38;5;66;03m# 5 years for training\u001b[39;00m\n\u001b[32m     26\u001b[39m     test_horizon=\u001b[32m12\u001b[39m,       \u001b[38;5;66;03m# 1 year for testing\u001b[39;00m\n\u001b[32m     27\u001b[39m     model_name=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLasso(alpha=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_alpha\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3g\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     28\u001b[39m     hac_lag=\u001b[32m0\u001b[39m\n\u001b[32m     29\u001b[39m )\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Lasso Rolling Window Results ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m display(lasso_results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[157]\u001b[39m\u001b[32m, line 108\u001b[39m, in \u001b[36mevaluate_model_rolling\u001b[39m\u001b[34m(model, X, y, dates, train_window, test_horizon, step, scaler, model_name, hac_lag, expanding, dropna)\u001b[39m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m    106\u001b[39m X_tr, y_tr = X_tr[tr_mask, :], y_tr[tr_mask]\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m te_mask = np.isfinite(y_te)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m te_mask.sum() == \u001b[32m0\u001b[39m:\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === Lasso Regression with Rolling Evaluation ===\n",
    "\n",
    "# 1) Choose alpha via CV (no shuffle to respect time ordering)\n",
    "alphas = np.logspace(-4, 1, 40)  # 1e-4 â€¦ 10\n",
    "lasso_cv = LassoCV(\n",
    "    alphas=alphas, \n",
    "    cv=5, \n",
    "    fit_intercept=True, \n",
    "    max_iter=5000, \n",
    "    n_jobs=-1, \n",
    "    random_state=42\n",
    ")\n",
    "lasso_cv.fit(X_std, y)\n",
    "best_alpha = float(lasso_cv.alpha_)\n",
    "print(f\"[LassoCV] best alpha = {best_alpha:.6g}\")\n",
    "\n",
    "# 2) Define Lasso with best alpha\n",
    "lasso = Lasso(alpha=best_alpha, fit_intercept=True, max_iter=5000, random_state=42)\n",
    "\n",
    "# 3) Rolling evaluation (example: 60-month rolling window, 12-month test window)\n",
    "lasso_results, lasso_details = evaluate_model_rolling(\n",
    "    model=lasso,\n",
    "    X=X_std,              # full standardized features\n",
    "    y=y,                  # full target returns\n",
    "    train_window=60,       # 5 years for training\n",
    "    test_horizon=12,       # 1 year for testing\n",
    "    model_name=f\"Lasso(alpha={best_alpha:.3g})\",\n",
    "    hac_lag=0\n",
    ")\n",
    "\n",
    "print(\"=== Lasso Rolling Window Results ===\")\n",
    "display(lasso_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "333dd43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonzero coefficients: 0 / 7\n",
      "\n",
      "=== Lasso Coefficients (standardized) ===\n",
      "    feature  coef_std\n",
      "      mktrf       0.0\n",
      "        smb       0.0\n",
      "        hml      -0.0\n",
      " gs10_yield      -0.0\n",
      "term_spread      -0.0\n",
      "  div_yield       0.0\n",
      " def_spread      -0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3) Inspect sparsity and coefficients (standardized features)\n",
    "coef = lasso_model.coef_\n",
    "nz_mask = coef != 0\n",
    "n_nz = int(nz_mask.sum())\n",
    "print(f\"Nonzero coefficients: {n_nz} / {len(coef)}\")\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"coef_std\": coef\n",
    "}).sort_values(\"coef_std\", key=lambda s: s.abs(), ascending=False)\n",
    "\n",
    "print(\"\\n=== Lasso Coefficients (standardized) ===\")\n",
    "print(coef_df.to_string(index=False))\n",
    "\n",
    "# 4) Store results\n",
    "all_results.append(lasso_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "66b57470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RidgeCV] best alpha = 10000\n",
      "=== Ridge Test Results ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\finnlp\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>HitRatio</th>\n",
       "      <th>FitTime (s)</th>\n",
       "      <th>R2_OOS</th>\n",
       "      <th>DM_stat</th>\n",
       "      <th>DM_p(two)</th>\n",
       "      <th>DM_p(one)</th>\n",
       "      <th>CW_stat</th>\n",
       "      <th>CW_p(one)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge(alpha=1e+04)</td>\n",
       "      <td>0.046803</td>\n",
       "      <td>0.036397</td>\n",
       "      <td>0.7128</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.000325</td>\n",
       "      <td>-0.586</td>\n",
       "      <td>0.5578</td>\n",
       "      <td>0.7211</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>0.7129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model      RMSE       MAE  HitRatio  FitTime (s)    R2_OOS  \\\n",
       "0  Ridge(alpha=1e+04)  0.046803  0.036397    0.7128        0.005 -0.000325   \n",
       "\n",
       "   DM_stat  DM_p(two)  DM_p(one)  CW_stat  CW_p(one)  \n",
       "0   -0.586     0.5578     0.7211   -0.562     0.7129  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Ridge Regression ===\n",
    "\n",
    "# 1) Choose alpha via CV\n",
    "alphas = np.logspace(-4, 4, 50)  # from 1e-4 to 1e4\n",
    "ridge_cv = RidgeCV(alphas=alphas, store_cv_values=False)\n",
    "ridge_cv.fit(X_train_std, y_train)\n",
    "best_alpha = float(ridge_cv.alpha_)\n",
    "print(f\"[RidgeCV] best alpha = {best_alpha:.6g}\")\n",
    "\n",
    "# 4) Refit Ridge with best alpha\n",
    "ridge = Ridge(alpha=best_alpha, fit_intercept=True, random_state=42)\n",
    "ridge_results, ridge_model = evaluate_model(\n",
    "    ridge, X_train_std, y_train, X_test_std, y_test, model_name=f\"Ridge(alpha={best_alpha:.3g})\", hac_lag=0\n",
    ")\n",
    "\n",
    "print(\"=== Ridge Test Results ===\")\n",
    "display(ridge_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c5a84636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ridge Coefficients (standardized) ===\n",
      "    feature  coef_std\n",
      "  div_yield  0.000095\n",
      "      mktrf  0.000091\n",
      " def_spread -0.000075\n",
      "        hml -0.000050\n",
      " gs10_yield -0.000045\n",
      "        smb  0.000037\n",
      "term_spread -0.000002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2) Print coefficients (standardized)\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"coef_std\": ridge_model.coef_\n",
    "}).sort_values(\"coef_std\", key=lambda s: s.abs(), ascending=False)\n",
    "\n",
    "print(\"\\n=== Ridge Coefficients (standardized) ===\")\n",
    "print(coef_df.to_string(index=False))\n",
    "\n",
    "# 3) Store results\n",
    "all_results.append(ridge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d7760d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ElasticNetCV] best alpha = 10, best l1_ratio = 0.1\n",
      "=== Elastic Net Test Results ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>HitRatio</th>\n",
       "      <th>FitTime (s)</th>\n",
       "      <th>R2_OOS</th>\n",
       "      <th>DM_stat</th>\n",
       "      <th>DM_p(two)</th>\n",
       "      <th>DM_p(one)</th>\n",
       "      <th>CW_stat</th>\n",
       "      <th>CW_p(one)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ElasticNet(alpha=10, l1=0.1)</td>\n",
       "      <td>0.046795</td>\n",
       "      <td>0.036413</td>\n",
       "      <td>0.7128</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model      RMSE       MAE  HitRatio  FitTime (s)  \\\n",
       "0  ElasticNet(alpha=10, l1=0.1)  0.046795  0.036413    0.7128        0.001   \n",
       "\n",
       "   R2_OOS  DM_stat  DM_p(two)  DM_p(one)  CW_stat  CW_p(one)  \n",
       "0     0.0      NaN        NaN        NaN      NaN        NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Elastic Net Regression ===\n",
    "# 1) Hyperparameter search (CV) for alpha and l1_ratio\n",
    "alphas = np.logspace(-4, 1, 40)                 # 1e-4 â€¦ 10\n",
    "l1_grid = [0.1, 0.3, 0.5, 0.7, 0.9]             # blend between Ridge(0) and Lasso(1)\n",
    "enet_cv = ElasticNetCV(\n",
    "    alphas=alphas,\n",
    "    l1_ratio=l1_grid,\n",
    "    cv=5,\n",
    "    max_iter=10000,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    fit_intercept=True\n",
    ")\n",
    "enet_cv.fit(X_train_std, y_train)\n",
    "\n",
    "best_alpha = float(enet_cv.alpha_)\n",
    "best_l1    = float(enet_cv.l1_ratio_)\n",
    "print(f\"[ElasticNetCV] best alpha = {best_alpha:.6g}, best l1_ratio = {best_l1:.3g}\")\n",
    "\n",
    "# 2) Refit Elastic Net using the best params (for fair FitTime measurement)\n",
    "enet = ElasticNet(\n",
    "    alpha=best_alpha,\n",
    "    l1_ratio=best_l1,\n",
    "    max_iter=10000,\n",
    "    fit_intercept=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "enet_results, enet_model = evaluate_model(\n",
    "    enet, X_train_std, y_train, X_test_std, y_test,\n",
    "    model_name=f\"ElasticNet(alpha={best_alpha:.3g}, l1={best_l1:.2g})\"\n",
    ")\n",
    "\n",
    "print(\"=== Elastic Net Test Results ===\")\n",
    "display(enet_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "bd0460d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonzero coefficients: 0 / 7\n",
      "\n",
      "=== Elastic Net Coefficients (standardized) ===\n",
      "    feature  coef_std\n",
      "      mktrf       0.0\n",
      "        smb       0.0\n",
      "        hml      -0.0\n",
      " gs10_yield      -0.0\n",
      "term_spread      -0.0\n",
      "  div_yield       0.0\n",
      " def_spread      -0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3) Coefficient inspection (standardized features)\n",
    "coef = enet_model.coef_\n",
    "nz_mask = coef != 0\n",
    "print(f\"Nonzero coefficients: {int(nz_mask.sum())} / {len(coef)}\")\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"coef_std\": coef\n",
    "}).sort_values(\"coef_std\", key=lambda s: s.abs(), ascending=False)\n",
    "\n",
    "print(\"\\n=== Elastic Net Coefficients (standardized) ===\")\n",
    "print(coef_df.to_string(index=False))\n",
    "\n",
    "# 4) Store results for later comparison\n",
    "all_results.append(enet_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab7c7ba",
   "metadata": {},
   "source": [
    "## Random Forest: Intuition and Application\n",
    "\n",
    "**Random Forest (RF)** is an ensemble learning method introduced by Breiman (2001).  \n",
    "It builds on decision trees but improves stability and accuracy through **bagging** and **random feature selection**.\n",
    "\n",
    "- **Decision Tree Basics**:  \n",
    "  Splits data recursively into regions based on predictor values.  \n",
    "  Simple and interpretable, but prone to **overfitting** and high variance.\n",
    "\n",
    "- **Random Forest Mechanics**:  \n",
    "  - **Bagging**: Each tree is trained on a bootstrap sample of the data.  \n",
    "  - **Feature Subsampling**: At each split, only a random subset of features is considered.  \n",
    "  - **Ensemble Prediction**:  \n",
    "    - Regression: average of predictions across trees.  \n",
    "    - Classification: majority vote across trees.  \n",
    "\n",
    "- **Strengths**:  \n",
    "  - Captures **nonlinearities** and **interactions** automatically.  \n",
    "  - Robust to noise and overfitting by averaging many decorrelated trees.  \n",
    "  - Scales well to high-dimensional datasets; training is parallelizable.  \n",
    "  - Provides **feature importance** measures useful in finance.\n",
    "\n",
    "- **Weaknesses**:  \n",
    "  - Acts like a **black box**; less interpretable than OLS or Lasso.  \n",
    "  - May be biased toward strong predictors if not tuned carefully.  \n",
    "  - Less efficient for sparse, very high-dimensional data.\n",
    "\n",
    "**Finance Takeaway**:  \n",
    "Random Forest is a strong off-the-shelf model for stock return prediction, especially when relationships are **nonlinear** and **noisy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8f1ca44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest Test Results ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>HitRatio</th>\n",
       "      <th>FitTime (s)</th>\n",
       "      <th>R2_OOS</th>\n",
       "      <th>DM_stat</th>\n",
       "      <th>DM_p(two)</th>\n",
       "      <th>DM_p(one)</th>\n",
       "      <th>CW_stat</th>\n",
       "      <th>CW_p(one)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForest(n=300)</td>\n",
       "      <td>0.049196</td>\n",
       "      <td>0.037725</td>\n",
       "      <td>0.6064</td>\n",
       "      <td>0.2782</td>\n",
       "      <td>-0.105252</td>\n",
       "      <td>-2.232</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.9872</td>\n",
       "      <td>-1.474</td>\n",
       "      <td>0.9298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model      RMSE       MAE  HitRatio  FitTime (s)    R2_OOS  \\\n",
       "0  RandomForest(n=300)  0.049196  0.037725    0.6064       0.2782 -0.105252   \n",
       "\n",
       "   DM_stat  DM_p(two)  DM_p(one)  CW_stat  CW_p(one)  \n",
       "0   -2.232     0.0256     0.9872   -1.474     0.9298  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1) Define RF model (sensible defaults for classroom demo)\n",
    "n_estimaters = 300\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=n_estimaters,\n",
    "    max_depth=None,          # let trees grow; adjust if overfitting\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',     # typical for tabular data\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2) Fit & evaluate\n",
    "rf_results, rf_model = evaluate_model(\n",
    "    rf, X_train, y_train, X_test, y_test, model_name=f\"RandomForest(n={n_estimaters})\"\n",
    ")\n",
    "\n",
    "print(\"=== Random Forest Test Results ===\")\n",
    "display(rf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "48a38a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Random Forest Feature Importances ===\n",
      "    feature  importance\n",
      "      mktrf    0.161123\n",
      "        hml    0.157534\n",
      " def_spread    0.152966\n",
      "  div_yield    0.145641\n",
      "        smb    0.141933\n",
      " gs10_yield    0.125534\n",
      "term_spread    0.115268\n"
     ]
    }
   ],
   "source": [
    "# 3) Feature importances\n",
    "fi = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"importance\": rf_model.feature_importances_\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(\"\\n=== Random Forest Feature Importances ===\")\n",
    "print(fi.to_string(index=False))\n",
    "\n",
    "# 4) Store results\n",
    "all_results.append(rf_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501c84bb",
   "metadata": {},
   "source": [
    "## Gradient Boosting Variants: XGBoost and LightGBM\n",
    "\n",
    "**Gradient Boosting** builds trees sequentially, where each new tree corrects the errors of the previous ones.  \n",
    "Among its modern implementations, **XGBoost** and **LightGBM** are industry standards for structured/tabular data.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ XGBoost (Chen & Guestrin, 2016)\n",
    "- **Growth Strategy**: Level-wise (splits nodes layer by layer).\n",
    "- **Strengths**:\n",
    "  - Strong regularization (L1/L2 penalties) â†’ helps prevent overfitting.\n",
    "  - Stable and widely used in Kaggle competitions and finance.\n",
    "  - Flexible with custom loss functions and objectives.\n",
    "- **Limitations**:\n",
    "  - Training can be slower on very large datasets compared to LightGBM.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ LightGBM (Ke et al., 2017)\n",
    "- **Growth Strategy**: Leaf-wise with depth constraints.\n",
    "- **Strengths**:\n",
    "  - Very **fast and memory-efficient** (histogram-based splitting).\n",
    "  - Handles large datasets and high-dimensional features effectively.\n",
    "  - Supports categorical variables natively.\n",
    "- **Limitations**:\n",
    "  - Leaf-wise growth may **overfit** small datasets if depth is not controlled.\n",
    "  - Requires careful tuning of learning rate, max depth, and regularization.\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison\n",
    "| Feature               | XGBoost                     | LightGBM                       |\n",
    "|-----------------------|-----------------------------|--------------------------------|\n",
    "| Tree Growth           | Level-wise                  | Leaf-wise                      |\n",
    "| Speed & Memory        | Moderate                    | Very fast, memory-efficient    |\n",
    "| Regularization        | Strong (L1/L2)              | Relies more on depth control   |\n",
    "| Robustness            | Stable, widely adopted      | Faster but risk of overfitting |\n",
    "| Use Case in Finance   | Robust baseline             | Large-scale, high-frequency data|\n",
    "\n",
    "---\n",
    "\n",
    "**Finance Takeaway**:  \n",
    "Both are strong choices for **stock return prediction**.  \n",
    "- Use **XGBoost** for stability and interpretability.  \n",
    "- Use **LightGBM** when dataset is **large and high-dimensional** with speed requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5369ed09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== XGBoost Test Results ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>HitRatio</th>\n",
       "      <th>FitTime (s)</th>\n",
       "      <th>R2_OOS</th>\n",
       "      <th>DM_stat</th>\n",
       "      <th>DM_p(two)</th>\n",
       "      <th>DM_p(one)</th>\n",
       "      <th>CW_stat</th>\n",
       "      <th>CW_p(one)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost(500, lr=0.05)</td>\n",
       "      <td>0.050096</td>\n",
       "      <td>0.037929</td>\n",
       "      <td>0.5745</td>\n",
       "      <td>0.3712</td>\n",
       "      <td>-0.146057</td>\n",
       "      <td>-1.611</td>\n",
       "      <td>0.1071</td>\n",
       "      <td>0.9464</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.5674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model      RMSE       MAE  HitRatio  FitTime (s)    R2_OOS  \\\n",
       "0  XGBoost(500, lr=0.05)  0.050096  0.037929    0.5745       0.3712 -0.146057   \n",
       "\n",
       "   DM_stat  DM_p(two)  DM_p(one)  CW_stat  CW_p(one)  \n",
       "0   -1.611     0.1071     0.9464    -0.17     0.5674  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === XGBoost Regression ===\n",
    "# 1) Define model (robust classroom defaults)\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,           # L2\n",
    "    reg_alpha=0.0,            # L1 (set >0 to encourage sparsity)\n",
    "    objective=\"reg:squarederror\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    tree_method=\"hist\"        # fast & memory-efficient\n",
    ")\n",
    "\n",
    "# 2) Fit & evaluate\n",
    "xgb_results, xgb_model = evaluate_model(\n",
    "    xgb, X_train, y_train, X_test, y_test, model_name=\"XGBoost(500, lr=0.05)\"\n",
    ")\n",
    "\n",
    "print(\"=== XGBoost Test Results ===\")\n",
    "display(xgb_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c5f5d96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== XGBoost Feature Importances ===\n",
      "    feature  importance\n",
      " gs10_yield    0.176990\n",
      " def_spread    0.173125\n",
      "  div_yield    0.144114\n",
      "        hml    0.136003\n",
      "      mktrf    0.132355\n",
      "        smb    0.121767\n",
      "term_spread    0.115646\n"
     ]
    }
   ],
   "source": [
    "# 3) Feature importances\n",
    "xgb_fi = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"importance\": xgb_model.feature_importances_\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(\"\\n=== XGBoost Feature Importances ===\")\n",
    "print(xgb_fi.to_string(index=False))\n",
    "\n",
    "# 4) Store results\n",
    "all_results.append(xgb_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "573acfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000205 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 618\n",
      "[LightGBM] [Info] Number of data points in the train set: 287, number of used features: 7\n",
      "[LightGBM] [Info] Start training from score 0.006022\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "=== LightGBM Test Results ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\finnlp\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>HitRatio</th>\n",
       "      <th>FitTime (s)</th>\n",
       "      <th>R2_OOS</th>\n",
       "      <th>DM_stat</th>\n",
       "      <th>DM_p(two)</th>\n",
       "      <th>DM_p(one)</th>\n",
       "      <th>CW_stat</th>\n",
       "      <th>CW_p(one)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM(600, lr=0.05)</td>\n",
       "      <td>0.051817</td>\n",
       "      <td>0.040891</td>\n",
       "      <td>0.5532</td>\n",
       "      <td>0.3247</td>\n",
       "      <td>-0.226146</td>\n",
       "      <td>-1.818</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.9655</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model      RMSE       MAE  HitRatio  FitTime (s)  \\\n",
       "0  LightGBM(600, lr=0.05)  0.051817  0.040891    0.5532       0.3247   \n",
       "\n",
       "     R2_OOS  DM_stat  DM_p(two)  DM_p(one)  CW_stat  CW_p(one)  \n",
       "0 -0.226146   -1.818      0.069     0.9655      0.2     0.4206  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === LightGBM Regression ===\n",
    "# 1) Define model (fast classroom defaults)\n",
    "lgbm = lgb.LGBMRegressor(\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    max_depth=-1,            # no explicit depth limit; controlled by num_leaves\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    reg_alpha=0.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 2) Fit & evaluate\n",
    "lgbm_results, lgbm_model = evaluate_model(\n",
    "    lgbm, X_train, y_train, X_test, y_test, model_name=\"LightGBM(600, lr=0.05)\"\n",
    ")\n",
    "\n",
    "print(\"=== LightGBM Test Results ===\")\n",
    "display(lgbm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "81586fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LightGBM Feature Importances (gain) ===\n",
      "    feature  importance\n",
      "      mktrf    0.909059\n",
      "  div_yield    0.772846\n",
      " gs10_yield    0.743791\n",
      "        hml    0.654102\n",
      " def_spread    0.641197\n",
      "        smb    0.619286\n",
      "term_spread    0.470007\n"
     ]
    }
   ],
   "source": [
    "# 3) Feature importances (gain-based is often more informative than split-count)\n",
    "lgbm_fi = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"importance\": lgbm_model.booster_.feature_importance(importance_type='gain')\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(\"\\n=== LightGBM Feature Importances (gain) ===\")\n",
    "print(lgbm_fi.to_string(index=False))\n",
    "\n",
    "# 4) Store results\n",
    "all_results.append(lgbm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ed5dc4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Summary of All Model Results ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>HitRatio</th>\n",
       "      <th>FitTime (s)</th>\n",
       "      <th>R2_OOS</th>\n",
       "      <th>DM_stat</th>\n",
       "      <th>DM_p(two)</th>\n",
       "      <th>DM_p(one)</th>\n",
       "      <th>CW_stat</th>\n",
       "      <th>CW_p(one)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OLS</td>\n",
       "      <td>0.047272</td>\n",
       "      <td>0.035950</td>\n",
       "      <td>0.7021</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>-0.020472</td>\n",
       "      <td>-0.758</td>\n",
       "      <td>0.4486</td>\n",
       "      <td>0.7757</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.4441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasso(alpha=10)</td>\n",
       "      <td>0.046795</td>\n",
       "      <td>0.036413</td>\n",
       "      <td>0.7128</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ridge(alpha=1e+04)</td>\n",
       "      <td>0.046803</td>\n",
       "      <td>0.036397</td>\n",
       "      <td>0.7128</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>-0.000325</td>\n",
       "      <td>-0.586</td>\n",
       "      <td>0.5578</td>\n",
       "      <td>0.7211</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>0.7129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ElasticNet(alpha=10, l1=0.1)</td>\n",
       "      <td>0.046795</td>\n",
       "      <td>0.036413</td>\n",
       "      <td>0.7128</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForest(n=100)</td>\n",
       "      <td>0.049449</td>\n",
       "      <td>0.038128</td>\n",
       "      <td>0.5851</td>\n",
       "      <td>0.0980</td>\n",
       "      <td>-0.116634</td>\n",
       "      <td>-2.429</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.9924</td>\n",
       "      <td>-1.530</td>\n",
       "      <td>0.9370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForest(n=300)</td>\n",
       "      <td>0.049196</td>\n",
       "      <td>0.037725</td>\n",
       "      <td>0.6064</td>\n",
       "      <td>0.2782</td>\n",
       "      <td>-0.105252</td>\n",
       "      <td>-2.232</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.9872</td>\n",
       "      <td>-1.474</td>\n",
       "      <td>0.9298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGBoost(500, lr=0.05)</td>\n",
       "      <td>0.050096</td>\n",
       "      <td>0.037929</td>\n",
       "      <td>0.5745</td>\n",
       "      <td>0.3712</td>\n",
       "      <td>-0.146057</td>\n",
       "      <td>-1.611</td>\n",
       "      <td>0.1071</td>\n",
       "      <td>0.9464</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>0.5674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LightGBM(600, lr=0.05)</td>\n",
       "      <td>0.051817</td>\n",
       "      <td>0.040891</td>\n",
       "      <td>0.5532</td>\n",
       "      <td>0.3247</td>\n",
       "      <td>-0.226146</td>\n",
       "      <td>-1.818</td>\n",
       "      <td>0.0690</td>\n",
       "      <td>0.9655</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.4206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model      RMSE       MAE  HitRatio  FitTime (s)  \\\n",
       "0                           OLS  0.047272  0.035950    0.7021       0.0020   \n",
       "1               Lasso(alpha=10)  0.046795  0.036413    0.7128       0.0010   \n",
       "2            Ridge(alpha=1e+04)  0.046803  0.036397    0.7128       0.0050   \n",
       "3  ElasticNet(alpha=10, l1=0.1)  0.046795  0.036413    0.7128       0.0010   \n",
       "4           RandomForest(n=100)  0.049449  0.038128    0.5851       0.0980   \n",
       "5           RandomForest(n=300)  0.049196  0.037725    0.6064       0.2782   \n",
       "6         XGBoost(500, lr=0.05)  0.050096  0.037929    0.5745       0.3712   \n",
       "7        LightGBM(600, lr=0.05)  0.051817  0.040891    0.5532       0.3247   \n",
       "\n",
       "     R2_OOS  DM_stat  DM_p(two)  DM_p(one)  CW_stat  CW_p(one)  \n",
       "0 -0.020472   -0.758     0.4486     0.7757    0.141     0.4441  \n",
       "1  0.000000      NaN        NaN        NaN      NaN        NaN  \n",
       "2 -0.000325   -0.586     0.5578     0.7211   -0.562     0.7129  \n",
       "3  0.000000      NaN        NaN        NaN      NaN        NaN  \n",
       "4 -0.116634   -2.429     0.0152     0.9924   -1.530     0.9370  \n",
       "5 -0.105252   -2.232     0.0256     0.9872   -1.474     0.9298  \n",
       "6 -0.146057   -1.611     0.1071     0.9464   -0.170     0.5674  \n",
       "7 -0.226146   -1.818     0.0690     0.9655    0.200     0.4206  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=== Summary of All Model Results ===\")\n",
    "final_summary = pd.concat(all_results, ignore_index=True)\n",
    "display(final_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
