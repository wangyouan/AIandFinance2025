{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a737010b",
   "metadata": {},
   "source": [
    "# Lecture 4: Simple Algorithms for Stock Return Prediction\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will implement and compare **four key machine learning models** for stock return prediction:\n",
    "\n",
    "1. **Linear Regression (OLS)** – the baseline linear model.\n",
    "2. **Regularized Regression (Lasso, Ridge, Elastic Net)** – improved generalization in high-dimensional, noisy settings.\n",
    "3. **Random Forest (RF)** – nonlinear, ensemble method robust to noise.\n",
    "4. **Gradient Boosting (GBDT, XGBoost, LightGBM)** – sequential ensemble method, industry standard for tabular prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Machine Learning for Stock Prediction?\n",
    "- Stock returns are **noisy** and often weakly predictable.  \n",
    "- Firm characteristics can be **high-dimensional** and correlated.  \n",
    "- Machine learning models help capture **nonlinear patterns** and avoid overfitting via regularization and ensembles.\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluation Metrics\n",
    "We will compare models using the following metrics:\n",
    "\n",
    "- **RMSE (Root Mean Squared Error):** accuracy, penalizes large errors.  \n",
    "- **MAE (Mean Absolute Error):** average error magnitude, robust to outliers.  \n",
    "- **Hit Ratio:** percentage of correct up/down predictions.  \n",
    "- **Fit Time:** computational efficiency of model training.  \n",
    "\n",
    "> In finance, **out-of-sample performance** is the gold standard — models must be evaluated on unseen data to ensure predictive power.\n",
    "\n",
    "---\n",
    "\n",
    "### Notebook Workflow\n",
    "1. Load and preprocess financial data.  \n",
    "2. Train models using historical features.  \n",
    "3. Evaluate models with RMSE, MAE, Hit Ratio, and training time.  \n",
    "4. Compare results to highlight **strengths and limitations** of each method.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9093671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Requirements / Imports ===\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning - scikit-learn\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Gradient Boosting - external libraries\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Timing\n",
    "import time\n",
    "\n",
    "# Display settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "plt.rcParams[\"axes.titlesize\"] = 14\n",
    "plt.rcParams[\"axes.labelsize\"] = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c6432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\finnlp\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:672: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs2, **kwargs)\n",
      "d:\\anaconda3\\envs\\finnlp\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:672: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs2, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# === Data Loading from WRDS ===\n",
    "import os\n",
    "import wrds\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "wrds_user = os.getenv(\"WRDS_USER\")\n",
    "\n",
    "# Connect to WRDS\n",
    "db = wrds.Connection(wrds_username=wrds_user)\n",
    "\n",
    "# ---------------------------\n",
    "# 1. CRSP Monthly Stock Data\n",
    "# ---------------------------\n",
    "crsp_msf = db.raw_sql(\"\"\"\n",
    "    select a.permno, a.date, a.ret, a.vol, a.shrout, a.prc,\n",
    "           b.ticker, b.comnam\n",
    "    from crsp.msf as a\n",
    "    left join crsp.msenames as b\n",
    "    on a.permno=b.permno\n",
    "    and b.namedt <= a.date\n",
    "    and a.date <= b.nameendt\n",
    "    where a.date between '2000-01-01' and '2024-12-31'\n",
    "\"\"\")\n",
    "\n",
    "# Clean CRSP\n",
    "crsp_msf['date'] = pd.to_datetime(crsp_msf['date'])\n",
    "crsp_msf['ret'] = pd.to_numeric(crsp_msf['ret'], errors='coerce')\n",
    "crsp_msf['me'] = crsp_msf['prc'].abs() * crsp_msf['shrout']   # Market equity\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Fama-French Factors (Monthly)\n",
    "# ---------------------------\n",
    "ff_factors = db.raw_sql(\"\"\"\n",
    "    select date, mktrf, smb, hml, rf\n",
    "    from ff.factors_monthly\n",
    "    where date between '2000-01-01' and '2024-12-31'\n",
    "\"\"\")\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Compustat Fundamentals (Quarterly) — build 10 features\n",
    "# ---------------------------\n",
    "# Pull key quarterly fields from comp.fundq\n",
    "compq = db.raw_sql(\"\"\"\n",
    "    select gvkey, datadate,\n",
    "           atq,        -- total assets (quarter)\n",
    "           ceqq,       -- common equity (quarter)\n",
    "           revtq,      -- revenue (total)\n",
    "           cogsq,      -- cost of goods sold\n",
    "           ibq,        -- income before extraordinary items\n",
    "           dlttq,      -- long-term debt\n",
    "           dlcq,       -- debt in current liabilities (short-term debt)\n",
    "           capxy,      -- capital expenditures\n",
    "           cheq,       -- cash & equivalents\n",
    "           actq,       -- current assets\n",
    "           lctq        -- current liabilities\n",
    "    from comp.fundq\n",
    "    where indfmt = 'INDL'\n",
    "      and datafmt = 'STD'\n",
    "      and popsrc = 'D'\n",
    "      and consol = 'C'\n",
    "      and datadate between '1999-01-01' and '2024-12-31'\n",
    "\"\"\")\n",
    "\n",
    "# Ensure datetime and reasonable ordering\n",
    "compq['datadate'] = pd.to_datetime(compq['datadate'], errors='coerce')\n",
    "compq = compq.sort_values(['gvkey', 'datadate']).rename(columns={\"capxy\": \"capxq\"})\n",
    "\n",
    "# Convert to numeric (in case of strings), keep as floats\n",
    "num_cols = ['atq','ceqq','revtq','cogsq','ibq','dlttq','dlcq','capxq','cheq','actq','lctq']\n",
    "for c in num_cols:\n",
    "    compq[c] = pd.to_numeric(compq[c], errors='coerce')\n",
    "\n",
    "# Create a quarter-end \"date\" aligned to month-end (for later merges to monthly)\n",
    "# Using PeriodIndex to guarantee quarter END timestamp\n",
    "compq['date'] = pd.PeriodIndex(compq['datadate'], freq='Q').to_timestamp(how='end')\n",
    "compq['date'] = compq['date'].dt.to_period('M').dt.to_timestamp('M')  # normalize to month-end\n",
    "\n",
    "# Build features (protect against divide-by-zero with where/clip)\n",
    "eps = 1e-12\n",
    "\n",
    "# 1) Size (log of assets)\n",
    "compq['size_log_at'] = np.log(compq['atq'].where(compq['atq'] > 0))\n",
    "\n",
    "# 2) Book-to-Market (quarterly proxy): common equity / assets\n",
    "compq['bm'] = (compq['ceqq'] / compq['atq']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 3) Leverage: (long-term + short-term debt) / assets\n",
    "compq['lev'] = ((compq['dlttq'].fillna(0) + compq['dlcq'].fillna(0)) / compq['atq']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 4) Sales growth QoQ: (revtq / L1(revtq) - 1)\n",
    "compq['revtq_l1'] = compq.groupby('gvkey')['revtq'].shift(1)\n",
    "compq['sales_g_qoq'] = (compq['revtq'] / compq['revtq_l1'] - 1).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 5) Sales growth YoY: (revtq / L4(revtq) - 1)\n",
    "compq['revtq_l4'] = compq.groupby('gvkey')['revtq'].shift(4)\n",
    "compq['sales_g_yoy'] = (compq['revtq'] / compq['revtq_l4'] - 1).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 6) Asset growth YoY: (atq / L4(atq) - 1)\n",
    "compq['atq_l4'] = compq.groupby('gvkey')['atq'].shift(4)\n",
    "compq['asset_g_yoy'] = (compq['atq'] / compq['atq_l4'] - 1).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 7) Profitability (ROA): ibq / assets (can also use lagged assets; here use contemporaneous as a simple proxy)\n",
    "compq['roa'] = (compq['ibq'] / compq['atq']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 8) Gross margin: (revtq - cogsq) / revtq\n",
    "compq['gross_margin'] = ((compq['revtq'] - compq['cogsq']) / compq['revtq']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 9) Capex-to-assets: capxq / atq\n",
    "compq['capex_to_assets'] = (compq['capxq'] / compq['atq']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 10) Cash-to-assets: cheq / atq\n",
    "compq['cash_to_assets'] = (compq['cheq'] / compq['atq']).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Keep only what we need going forward\n",
    "compq_features = compq[[\n",
    "    'gvkey', 'datadate', 'date',\n",
    "    'size_log_at', 'bm', 'lev',\n",
    "    'sales_g_qoq', 'sales_g_yoy', 'asset_g_yoy',\n",
    "    'roa', 'gross_margin', 'capex_to_assets', 'cash_to_assets'\n",
    "]].copy()\n",
    "\n",
    "# (Optional) De-duplicate quarterly rows per gvkey-datadate if needed\n",
    "compq_features = compq_features.drop_duplicates(subset=['gvkey', 'datadate'])\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "127aeb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 4. Merge Data (Permno / GVKEY link needed)\n",
    "# ---------------------------\n",
    "# Note: requires CRSP-Compustat linking table (ccmxpf_linktable)\n",
    "link_table = pd.read_sas(os.path.join('data', 'l4', 'ccmxpf_linktable.sas7bdat'), format='sas7bdat', encoding='utf-8').rename(\n",
    "    columns={'lpermno': 'permno'})\n",
    "\n",
    "link_table['linkdt'] = pd.to_datetime(link_table['linkdt'])\n",
    "link_table['linkenddt'] = pd.to_datetime(link_table['linkenddt'])\n",
    "\n",
    "# Keep common, high-quality link types and primaries\n",
    "link_keep_types = {'LC', 'LU'}   # standard/unique CRSP-permno ↔ Compustat-gvkey links\n",
    "link_keep_prim  = {'P', 'C'}     # prefer primary (P), then consolidated (C)\n",
    "\n",
    "link_table = link_table.loc[\n",
    "    link_table['linktype'].isin(link_keep_types) &\n",
    "    link_table['linkprim'].isin(link_keep_prim),\n",
    "    ['gvkey', 'permno', 'linkdt', 'linkenddt', 'linktype', 'linkprim']\n",
    "].copy()\n",
    "\n",
    "# Treat open-ended linkenddt as far-future to simplify interval filtering\n",
    "link_table['linkenddt'] = link_table['linkenddt'].fillna(pd.Timestamp('2099-12-31'))\n",
    "\n",
    "# 4.1 First, join CRSP to link table on permno, then keep only rows where the CRSP month lies within the valid link interval\n",
    "crsp_lnk = crsp_msf.merge(link_table, on='permno', how='left')\n",
    "mask_valid_interval = (crsp_lnk['date'] >= crsp_lnk['linkdt']) & (crsp_lnk['date'] <= crsp_lnk['linkenddt'])\n",
    "crsp_lnk = crsp_lnk.loc[mask_valid_interval].copy()\n",
    "\n",
    "# If multiple gvkeys match the same (permno, date), keep the \"best\" one:\n",
    "#   - Prefer linkprim=P over C; prefer linktype=LC over LU; if tied, keep the most recent linkdt\n",
    "prim_order = {'P': 0, 'C': 1}\n",
    "type_order = {'LC': 0, 'LU': 1}\n",
    "crsp_lnk['prim_rank'] = crsp_lnk['linkprim'].map(prim_order)\n",
    "crsp_lnk['type_rank'] = crsp_lnk['linktype'].map(type_order)\n",
    "\n",
    "crsp_lnk = (crsp_lnk\n",
    "    .sort_values(['permno', 'date', 'prim_rank', 'type_rank', 'linkdt'],\n",
    "                 ascending=[True, True, True, True, False])\n",
    "    .drop_duplicates(subset=['permno', 'date'], keep='first')\n",
    "    .drop(columns=['prim_rank', 'type_rank'])\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a85de018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final merged shape: (1805631, 32)\n",
      "   permno   gvkey       date    datem   dateq       ret   mktrf     smb  \\\n",
      "0   10001  012994 2000-01-31  2000-01  2000Q1 -0.044118 -0.0474  0.0516   \n",
      "1   10001  012994 2000-02-29  2000-02  2000Q1  0.015385  0.0246  0.2125   \n",
      "2   10001  012994 2000-03-31  2000-03  2000Q1 -0.015758  0.0521 -0.1741   \n",
      "3   10001  012994 2000-04-28  2000-04  2000Q2  0.011719 -0.0639   -0.06   \n",
      "4   10001  012994 2000-05-31  2000-05  2000Q2 -0.023166 -0.0439 -0.0608   \n",
      "\n",
      "      hml      rf  size_log_at        bm       lev  \n",
      "0 -0.0112  0.0041     3.916513  0.283086  0.435122  \n",
      "1 -0.0977  0.0043     3.916513  0.283086  0.435122  \n",
      "2   0.085  0.0047     3.916513  0.283086  0.435122  \n",
      "3  0.0645  0.0046     3.923022  0.276166  0.429154  \n",
      "4  0.0459   0.005     3.923022  0.276166  0.429154  \n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Build month and quarter keys; merge CRSP + FF (by month) + CompustatQ (by quarter)\n",
    "# ==========================================\n",
    "\n",
    "# --- 0) Normalize dates to pandas Timestamps (if not already) ---\n",
    "crsp_lnk['date'] = pd.to_datetime(crsp_lnk['date'], errors='coerce')\n",
    "ff_factors['date'] = pd.to_datetime(ff_factors['date'], errors='coerce')\n",
    "compq_features['datadate'] = pd.to_datetime(compq_features['datadate'], errors='coerce')\n",
    "compq_features['date'] = pd.to_datetime(compq_features['date'], errors='coerce')  # quarter end already\n",
    "\n",
    "# --- 1) Create merge keys ---\n",
    "# CRSP: month and quarter from the CRSP month-end date\n",
    "crsp_lnk['datem'] = crsp_lnk['date'].dt.to_period('M')  # YYYY-MM\n",
    "crsp_lnk['dateq'] = crsp_lnk['date'].dt.to_period('Q')  # YYYYQn\n",
    "\n",
    "# FF factors: merge on month (datem)\n",
    "ff_factors['datem'] = ff_factors['date'].dt.to_period('M')  # YYYY-MM\n",
    "\n",
    "# CompustatQ: create quarter key from the financial report date (prefer datadate’s fiscal quarter)\n",
    "# Using datadate ensures we do NOT leak future information before the filing quarter.\n",
    "compq_features['dateq'] = compq_features['datadate'].dt.to_period('Q')  # YYYYQn\n",
    "\n",
    "# Optional safety: ensure at most one Compustat row per (gvkey, dateq)\n",
    "compq_q = (compq_features\n",
    "           .sort_values(['gvkey', 'datadate'])\n",
    "           .drop_duplicates(subset=['gvkey', 'dateq']))\n",
    "\n",
    "# --- 2) Merge CRSP with Fama–French by month key (datem) ---\n",
    "crsp_ff = crsp_lnk.merge(\n",
    "    ff_factors[['datem', 'mktrf', 'smb', 'hml', 'rf']],\n",
    "    on='datem',\n",
    "    how='left',\n",
    "    validate='many_to_one'  # one factor vector per month\n",
    ")\n",
    "\n",
    "# --- 3) Merge CRSP(+FF) with CompustatQ by quarter key (dateq) and gvkey ---\n",
    "# Requires crsp_lnk to already have gvkey via the CCM link step\n",
    "final_data = crsp_ff.merge(\n",
    "    compq_q[['gvkey', 'dateq',\n",
    "             'size_log_at', 'bm', 'lev',\n",
    "             'sales_g_qoq', 'sales_g_yoy', 'asset_g_yoy',\n",
    "             'roa', 'gross_margin', 'capex_to_assets', 'cash_to_assets']],\n",
    "    on=['gvkey', 'dateq'],\n",
    "    how='left',\n",
    "    validate='many_to_one'  # one accounting record per gvkey per quarter\n",
    ")\n",
    "\n",
    "# --- 4) Optional: basic cleanup ---\n",
    "# Drop exact duplicates on (permno, date); keep first occurrence\n",
    "final_data = final_data.drop_duplicates(subset=['permno', 'date']).copy()\n",
    "\n",
    "# --- 5) Inspect result ---\n",
    "print(\"Final merged shape:\", final_data.shape)\n",
    "print(final_data[['permno','gvkey','date','datem','dateq',\n",
    "                  'ret','mktrf','smb','hml','rf',\n",
    "                  'size_log_at','bm','lev']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e63c029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows: 838,742 | Test rows: 298,470\n",
      "Train period: 2000-12-29 ~ 2017-12-29\n",
      "Test  period: 2018-01-31 ~ 2024-11-29\n"
     ]
    }
   ],
   "source": [
    "# === Preprocessing (monthly, out-of-sample safe) ===\n",
    "# ---------------------------\n",
    "# 0) Basic hygiene\n",
    "# ---------------------------\n",
    "df = final_data.copy()\n",
    "\n",
    "# Ensure date sorted\n",
    "df = df.sort_values(['permno', 'date'])\n",
    "\n",
    "acct_cols = ['size_log_at','bm','lev','sales_g_qoq','sales_g_yoy','asset_g_yoy',\n",
    "             'roa','gross_margin','capex_to_assets','cash_to_assets']\n",
    "for key in ['bm', 'lev', 'sales_g_qoq', 'sales_g_yoy', 'asset_g_yoy',\n",
    "            'roa', 'gross_margin', 'capex_to_assets', 'cash_to_assets']:\n",
    "    df[key] = df[key].astype(np.float64)\n",
    "\n",
    "# Keep core columns presence\n",
    "needed_cols = ['permno','date','ret','rf','me','mktrf','smb','hml'] + acct_cols\n",
    "\n",
    "for c in needed_cols:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f\"Missing required column: {c}. Please check data loading step.\")\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Target: next-month excess return\n",
    "# ---------------------------\n",
    "df['excess_ret'] = df['ret'] - df['rf']  # current month excess return\n",
    "df['excess_ret_fwd1'] = df.groupby('permno')['excess_ret'].shift(-1)  # predict next month\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Feature engineering (all lagged to avoid look-ahead)\n",
    "#    - Size (log ME) already as 'size' from Compustat; keep a CRSP-based log(ME) too for robustness\n",
    "#    - Book-to-Market 'bm' from Compustat\n",
    "#    - Momentum 12-2 (skip most recent month): sum of returns t-12..t-2\n",
    "#    - Volatility 6m: rolling std of monthly returns\n",
    "#    - Past market factor (lagged): mktrf_{t-1}\n",
    "# ---------------------------\n",
    "\n",
    "# CRSP-based log(ME)\n",
    "df['log_me'] = np.log(df['me'].replace({0: np.nan}))\n",
    "\n",
    "# Momentum 12-2:\n",
    "# comp_12 = (1+ret).rolling(12).prod() - 1\n",
    "# then subtract last month's return (t-1) to \"skip\" the most recent month\n",
    "comp_12 = df.groupby('permno')['ret'].transform(\n",
    "    lambda x: (1 + x).rolling(12).apply(np.prod, raw=True) - 1\n",
    ")\n",
    "last_m = df.groupby('permno', sort=False)['ret'].shift(1)\n",
    "df['mom_12_2'] = comp_12 - last_m\n",
    "\n",
    "# Volatility 6m (rolling std over last 6 months)\n",
    "df['vol_6m'] = df.groupby('permno', sort=False)['ret'] \\\n",
    "                 .transform(lambda x: x.rolling(6).std())\n",
    "\n",
    "# Lagged market factor (last month, global series)\n",
    "df = df.sort_values(['date'])  # ensure chronological order for global lag\n",
    "df['mktrf_lag1'] = df['mktrf'].shift(1)\n",
    "df['smb_lag1']   = df['smb'].shift(1)\n",
    "df['hml_lag1']   = df['hml'].shift(1)\n",
    "df['rf_lag1']    = df['rf'].shift(1)\n",
    "\n",
    "# Make sure fundamentals are carried forward within each stock (or use gvkey if preferred)\n",
    "df = df.sort_values(['permno', 'date'])\n",
    "df[['size_log_at', 'bm']] = df.groupby('permno')[['size_log_at', 'bm']].ffill()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Assemble feature set\n",
    "# ---------------------------\n",
    "feature_cols = [\n",
    "    # CRSP-derived\n",
    "    'log_me', 'mom_12_2', 'vol_6m',\n",
    "    # FF factors (lagged)\n",
    "    'mktrf_lag1', 'smb_lag1', 'hml_lag1', 'rf_lag1',\n",
    "    # Compustat quarterly fundamentals (carried forward)\n",
    "    'size_log_at', 'bm', 'lev', 'sales_g_qoq', 'sales_g_yoy',\n",
    "    'asset_g_yoy', 'roa', 'gross_margin', 'capex_to_assets', 'cash_to_assets'\n",
    "]\n",
    "\n",
    "# Drop rows where core target or features missing\n",
    "cols_needed_for_model = ['permno','date','excess_ret_fwd1'] + feature_cols\n",
    "df_model = df[cols_needed_for_model].replace([np.inf, -np.inf], np.nan).dropna(how='any')\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Winsorization (1% / 99%) on features to reduce tail sensitivity\n",
    "# ---------------------------\n",
    "def winsorize_df(dfin, cols, p=0.01):\n",
    "    d = dfin.copy()\n",
    "    for c in cols:\n",
    "        q_low, q_high = d[c].quantile([p, 1-p])\n",
    "        d[c] = d[c].clip(lower=q_low, upper=q_high)\n",
    "    return d\n",
    "\n",
    "df_model = winsorize_df(df_model, feature_cols, p=0.01)\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Temporal train/test split (expandable; here: fixed split)\n",
    "#    Train: 2005-01-01 ~ 2017-12-31\n",
    "#    Test : 2018-01-01 ~ 2024-12-31\n",
    "# ---------------------------\n",
    "train_end = pd.Timestamp('2017-12-31')\n",
    "test_start = pd.Timestamp('2018-01-01')\n",
    "\n",
    "train_mask = df_model['date'] <= train_end\n",
    "test_mask  = df_model['date'] >= test_start\n",
    "\n",
    "train_df = df_model.loc[train_mask].dropna(how='any')\n",
    "test_df  = df_model.loc[test_mask].dropna(how='any')\n",
    "\n",
    "# Shuffle is NOT used (time-series); keep order if needed for rolling evaluation.\n",
    "# Basic checks\n",
    "print(f\"Train rows: {len(train_df):,} | Test rows: {len(test_df):,}\")\n",
    "print(f\"Train period: {train_df['date'].min().date()} ~ {train_df['date'].max().date()}\")\n",
    "print(f\"Test  period: {test_df['date'].min().date()} ~ {test_df['date'].max().date()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "942c9987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "try:\n",
    "    from scipy.stats import norm\n",
    "    _HAS_SCIPY = True\n",
    "except Exception:\n",
    "    _HAS_SCIPY = False\n",
    "    \n",
    "def _nw_var(d, lag=0):\n",
    "    \"\"\"\n",
    "    Newey–West variance of the mean of d (loss differential), lag = q.\n",
    "    Returns var(mean(d)) = (gamma0 + 2*sum w_k*gamma_k)/T\n",
    "    \"\"\"\n",
    "    d = np.asarray(d, float)\n",
    "    T = len(d)\n",
    "    d = d - d.mean()\n",
    "    gamma0 = np.dot(d, d) / T\n",
    "    var = gamma0\n",
    "    for k in range(1, lag + 1):\n",
    "        w = 1.0 - k / (lag + 1.0)  # Bartlett weights\n",
    "        cov = np.dot(d[k:], d[:-k]) / T\n",
    "        var += 2.0 * w * cov\n",
    "    return var / T  # variance of the mean\n",
    "\n",
    "def dm_test(y_true, y_model, y_bench, lag=0):\n",
    "    \"\"\"\n",
    "    Diebold–Mariano test (squared-error loss) comparing model vs benchmark.\n",
    "    H0: E(loss_model - loss_bench) = 0  (no improvement)\n",
    "    Returns: (stat, p_two_sided, p_one_sided) where one-sided favors the model.\n",
    "    \"\"\"\n",
    "    e_m = y_true - y_model\n",
    "    e_b = y_true - y_bench\n",
    "    d = (e_b**2) - (e_m**2)  # positive => model improves over benchmark\n",
    "    m = np.mean(d)\n",
    "    v = _nw_var(d, lag=lag)\n",
    "    stat = m / sqrt(v) if v > 0 else np.nan\n",
    "    if _HAS_SCIPY:\n",
    "        p_two = 2.0 * (1.0 - norm.cdf(abs(stat)))\n",
    "        p_one = 1.0 - norm.cdf(stat)  # H1: model better (stat > 0)\n",
    "    else:\n",
    "        # simple normal approx via error function\n",
    "        from math import erf\n",
    "        def _phi(z): return 0.5 * (1.0 + erf(z / np.sqrt(2.0)))\n",
    "        p_two = 2.0 * (1.0 - _phi(abs(stat)))\n",
    "        p_one = 1.0 - _phi(stat)\n",
    "    return stat, p_two, p_one\n",
    "\n",
    "def clark_west_test(y_true, f_model, f_bench, lag=0):\n",
    "    \"\"\"\n",
    "    Clark–West (2007) test for nested models vs benchmark (squared-error loss).\n",
    "    Let e_m = y - f_model, e_b = y - f_bench.\n",
    "    Adjusted differential: d_t = e_b^2 - ( e_m^2 - (f_b - f_m)^2 )\n",
    "    H0: E(d_t) = 0 (no improvement); H1: E(d_t) > 0 (model improves).\n",
    "    Returns: (stat, p_one_sided)\n",
    "    \"\"\"\n",
    "    e_m = y_true - f_model\n",
    "    e_b = y_true - f_bench\n",
    "    d = (e_b**2) - (e_m**2 - (f_bench - f_model)**2)\n",
    "    m = np.mean(d)\n",
    "    v = _nw_var(d, lag=lag)\n",
    "    stat = m / sqrt(v) if v > 0 else np.nan\n",
    "    if _HAS_SCIPY:\n",
    "        p_one = 1.0 - norm.cdf(stat)\n",
    "    else:\n",
    "        from math import erf\n",
    "        def _phi(z): return 0.5 * (1.0 + erf(z / np.sqrt(2.0)))\n",
    "        p_one = 1.0 - _phi(stat)\n",
    "    return stat, p_one\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test,\n",
    "                   model_name=None, hac_lag=0):\n",
    "    \"\"\"\n",
    "    Fit and evaluate a model with RMSE/MAE/Hit/FitTime plus:\n",
    "      - R2_OOS vs. historical-mean benchmark\n",
    "      - DM test vs. benchmark (squared-error loss)\n",
    "      - CW test vs. benchmark (nested case)\n",
    "    Parameters\n",
    "    ----------\n",
    "    hac_lag : int\n",
    "        Newey–West lag q (use 0 for 1-step ahead; increase if overlapping horizons).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    results : pd.DataFrame  (one row, pretty-printable)\n",
    "    fitted_model : estimator\n",
    "    \"\"\"\n",
    "\n",
    "    # === 1) Fit & predict ===\n",
    "    t0 = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    fit_time = time.time() - t0\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # === 2) Benchmark: historical mean from TRAIN ===\n",
    "    mu_hat = float(np.mean(y_train))\n",
    "    y_bench = np.full_like(y_test, mu_hat, dtype=float)\n",
    "\n",
    "    # === 3) Standard metrics ===\n",
    "    rmse = float(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "    mae  = float(mean_absolute_error(y_test, y_pred))\n",
    "    hit  = float((np.sign(y_pred) == np.sign(y_test)).mean())\n",
    "\n",
    "    # === 4) OOS R^2 vs. benchmark ===\n",
    "    sse_model = float(np.sum((y_test - y_pred) ** 2))\n",
    "    sse_bench = float(np.sum((y_test - y_bench) ** 2))\n",
    "    r2_oos = 1.0 - sse_model / sse_bench if sse_bench > 0 else np.nan\n",
    "\n",
    "    # === 5) DM & CW tests ===\n",
    "    dm_stat, dm_p_two, dm_p_one = dm_test(y_test, y_pred, y_bench, lag=hac_lag)\n",
    "    cw_stat, cw_p_one = clark_west_test(y_test, y_pred, y_bench, lag=hac_lag)\n",
    "\n",
    "    # === 6) Pack results (tidy DataFrame) ===\n",
    "    results = pd.DataFrame([{\n",
    "        \"Model\": model_name or model.__class__.__name__,\n",
    "        \"RMSE\": round(rmse, 6),\n",
    "        \"MAE\": round(mae, 6),\n",
    "        \"HitRatio\": round(hit, 4),\n",
    "        \"FitTime (s)\": round(fit_time, 4),\n",
    "        \"R2_OOS\": round(r2_oos, 6),\n",
    "        \"DM_stat\": round(dm_stat, 3),\n",
    "        \"DM_p(two)\": round(dm_p_two, 4),\n",
    "        \"DM_p(one)\": round(dm_p_one, 4),\n",
    "        \"CW_stat\": round(cw_stat, 3),\n",
    "        \"CW_p(one)\": round(cw_p_one, 4)\n",
    "    }])\n",
    "\n",
    "    return results, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915eec63",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares (OLS)\n",
    "\n",
    "**Idea:**  \n",
    "Ordinary Least Squares (OLS) is the baseline linear regression method. It estimates the relationship between a dependent variable $y$ and explanatory variables $X$ by fitting a straight line (or hyperplane) that minimizes the **sum of squared errors**.\n",
    "\n",
    "**Model:**\n",
    "$$\n",
    "y_i = \\alpha + X_i^\\top \\beta + \\varepsilon_i\n",
    "$$\n",
    "\n",
    "- $y_i$: outcome (e.g., next-month excess return)  \n",
    "- $X_i$: vector of predictors (firm characteristics, factors)  \n",
    "- $\\beta$: coefficients to be estimated  \n",
    "- $\\varepsilon_i$: error term (unexplained part)\n",
    "\n",
    "**Estimation Principle:**  \n",
    "OLS chooses $\\hat{\\beta}$ to minimize\n",
    "$$\n",
    "\\min_{\\beta} \\sum_{i=1}^n \\left(y_i - X_i^\\top \\beta \\right)^2\n",
    "$$\n",
    "\n",
    "**Interpretation:**  \n",
    "- Each $\\beta_j$ shows how a one-unit change in predictor $X_j$ affects the expected outcome, holding others constant.  \n",
    "- Provides a simple and interpretable baseline.  \n",
    "\n",
    "**Limitations:**  \n",
    "- Assumes linear relationships.  \n",
    "- Sensitive to multicollinearity and outliers.  \n",
    "- May overfit when predictors are high-dimensional and noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "35975728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OLS Test Results ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>HitRatio</th>\n",
       "      <th>FitTime (s)</th>\n",
       "      <th>R2_OOS</th>\n",
       "      <th>DM_stat</th>\n",
       "      <th>DM_p(two)</th>\n",
       "      <th>DM_p(one)</th>\n",
       "      <th>CW_stat</th>\n",
       "      <th>CW_p(one)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OLS</td>\n",
       "      <td>0.215052</td>\n",
       "      <td>0.120753</td>\n",
       "      <td>0.4571</td>\n",
       "      <td>0.2757</td>\n",
       "      <td>-0.020313</td>\n",
       "      <td>-35.155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.278</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model      RMSE       MAE  HitRatio  FitTime (s)    R2_OOS  DM_stat  \\\n",
       "0   OLS  0.215052  0.120753    0.4571       0.2757 -0.020313  -35.155   \n",
       "\n",
       "   DM_p(two)  DM_p(one)  CW_stat  CW_p(one)  \n",
       "0        0.0        1.0   -6.278        1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1) Build matrices\n",
    "X_train = train_df[feature_cols].to_numpy(dtype='float64')\n",
    "y_train = train_df['excess_ret_fwd1'].to_numpy(dtype='float64')\n",
    "X_test  = test_df[feature_cols].to_numpy(dtype='float64')\n",
    "y_test  = test_df['excess_ret_fwd1'].to_numpy(dtype='float64')\n",
    "\n",
    "# 2) Standardize features (recommended for linear models)\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std  = scaler.transform(X_test)\n",
    "\n",
    "# 3) Fit and evaluate OLS\n",
    "ols = LinearRegression()\n",
    "ols_results, ols_model = evaluate_model(ols, X_train_std, y_train, X_test_std, y_test,\n",
    "                            model_name=\"OLS\", hac_lag=0)\n",
    "\n",
    "# 4) Print evaluation summary\n",
    "print(\"=== OLS Test Results ===\")\n",
    "display(ols_results)\n",
    "\n",
    "all_results = []\n",
    "all_results.append(ols_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45377a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7cef5ff",
   "metadata": {},
   "source": [
    "## Regularized Regression: Ridge, Lasso, and Elastic Net\n",
    "\n",
    "**Motivation:**  \n",
    "Ordinary Least Squares (OLS) can overfit when predictors are **high-dimensional**, **noisy**, or **highly correlated**.  \n",
    "Regularization methods add a penalty to the loss function, shrinking coefficients to improve generalization.\n",
    "\n",
    "---\n",
    "\n",
    "### Lasso Regression (L1 penalty)\n",
    "Lasso uses the **sum of absolute coefficients** as a penalty:\n",
    "$$\n",
    "\\min_{\\beta} \\sum_{i=1}^n (y_i - X_i^\\top \\beta)^2 + \\lambda \\sum_{j=1}^p |\\beta_j|\n",
    "$$\n",
    "\n",
    "- Performs **variable selection**: some coefficients shrink exactly to zero.  \n",
    "- Useful when we believe only a subset of predictors are truly important.  \n",
    "- Tends to select one variable from a group of correlated variables.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Ridge Regression (L2 penalty)\n",
    "Ridge shrinks coefficients by adding the **sum of squared coefficients** as a penalty:\n",
    "$$\n",
    "\\min_{\\beta} \\sum_{i=1}^n (y_i - X_i^\\top \\beta)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\n",
    "$$\n",
    "\n",
    "- Keeps all predictors, but shrinks them toward zero.  \n",
    "- Works well when predictors are correlated.  \n",
    "- Tuning parameter $\\lambda$ controls the amount of shrinkage.\n",
    "---\n",
    "\n",
    "### Elastic Net (L1 + L2 penalty)\n",
    "Elastic Net combines both L1 and L2 penalties:\n",
    "$$\n",
    "\\min_{\\beta} \\sum_{i=1}^n (y_i - X_i^\\top \\beta)^2 + \\lambda_1 \\sum_{j=1}^p |\\beta_j| + \\lambda_2 \\sum_{j=1}^p \\beta_j^2\n",
    "$$\n",
    "\n",
    "- Balances **shrinkage** (Ridge) and **variable selection** (Lasso).  \n",
    "- Works better than Lasso when predictors are highly correlated.  \n",
    "- Controlled by two hyperparameters: $\\lambda_1$ (L1 strength) and $\\lambda_2$ (L2 strength).\n",
    "\n",
    "---\n",
    "\n",
    "**Takeaway:**  \n",
    "- **Lasso** → performs automatic feature selection.  \n",
    "- **Ridge** → keeps all predictors, reduces variance.  \n",
    "- **Elastic Net** → compromise, often best in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "02a445a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LassoCV] best alpha = 0.0001\n",
      "=== Lasso Test Results ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>HitRatio</th>\n",
       "      <th>FitTime (s)</th>\n",
       "      <th>R2_OOS</th>\n",
       "      <th>DM_stat</th>\n",
       "      <th>DM_p(two)</th>\n",
       "      <th>DM_p(one)</th>\n",
       "      <th>CW_stat</th>\n",
       "      <th>CW_p(one)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lasso(alpha=0.0001)</td>\n",
       "      <td>0.215012</td>\n",
       "      <td>0.120678</td>\n",
       "      <td>0.4563</td>\n",
       "      <td>0.3938</td>\n",
       "      <td>-0.019935</td>\n",
       "      <td>-35.174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.71</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model      RMSE       MAE  HitRatio  FitTime (s)    R2_OOS  \\\n",
       "0  Lasso(alpha=0.0001)  0.215012  0.120678    0.4563       0.3938 -0.019935   \n",
       "\n",
       "   DM_stat  DM_p(two)  DM_p(one)  CW_stat  CW_p(one)  \n",
       "0  -35.174        0.0        1.0    -6.71        1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Lasso Regression ===\n",
    "# 1) Choose alpha via CV (no shuffle to respect time ordering)\n",
    "alphas = np.logspace(-4, 1, 40)  # 1e-4 … 10\n",
    "lasso_cv = LassoCV(alphas=alphas, cv=5, fit_intercept=True, max_iter=5000, n_jobs=-1, random_state=42)\n",
    "lasso_cv.fit(X_train_std, y_train)\n",
    "best_alpha = float(lasso_cv.alpha_)\n",
    "print(f\"[LassoCV] best alpha = {best_alpha:.6g}\")\n",
    "\n",
    "# 2) Refit Lasso with best alpha (so fit time is comparable to OLS)\n",
    "lasso = Lasso(alpha=best_alpha, fit_intercept=True, max_iter=5000, random_state=42)\n",
    "lasso_results, lasso_model = evaluate_model(\n",
    "    lasso, X_train_std, y_train, X_test_std, y_test, model_name=f\"Lasso(alpha={best_alpha:.3g})\", hac_lag=0\n",
    ")\n",
    "\n",
    "print(\"=== Lasso Test Results ===\")\n",
    "display(lasso_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "333dd43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonzero coefficients: 17 / 17\n",
      "\n",
      "=== Lasso Coefficients (standardized) ===\n",
      "        feature  coef_std\n",
      "         log_me -0.020943\n",
      "     mktrf_lag1  0.019261\n",
      "    size_log_at  0.014252\n",
      "            roa  0.011622\n",
      "       smb_lag1 -0.010128\n",
      " cash_to_assets  0.003872\n",
      "            lev -0.003657\n",
      "    sales_g_yoy  0.003583\n",
      "         vol_6m  0.003422\n",
      "             bm -0.002401\n",
      "       hml_lag1  0.001504\n",
      "    asset_g_yoy -0.001480\n",
      "capex_to_assets -0.001270\n",
      "       mom_12_2 -0.001204\n",
      "    sales_g_qoq  0.001198\n",
      "   gross_margin -0.000972\n",
      "        rf_lag1 -0.000648\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3) Inspect sparsity and coefficients (standardized features)\n",
    "coef = lasso_model.coef_\n",
    "nz_mask = coef != 0\n",
    "n_nz = int(nz_mask.sum())\n",
    "print(f\"Nonzero coefficients: {n_nz} / {len(coef)}\")\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"coef_std\": coef\n",
    "}).sort_values(\"coef_std\", key=lambda s: s.abs(), ascending=False)\n",
    "\n",
    "print(\"\\n=== Lasso Coefficients (standardized) ===\")\n",
    "print(coef_df.to_string(index=False))\n",
    "\n",
    "# 4) Store results\n",
    "all_results.append(lasso_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "66b57470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\finnlp\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:2385: FutureWarning: 'store_cv_values' is deprecated in version 1.5 and will be removed in 1.7. Use 'store_cv_results' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RidgeCV] best alpha = 159.986\n",
      "=== Ridge Test Results ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>HitRatio</th>\n",
       "      <th>FitTime (s)</th>\n",
       "      <th>R2_OOS</th>\n",
       "      <th>DM_stat</th>\n",
       "      <th>DM_p(two)</th>\n",
       "      <th>DM_p(one)</th>\n",
       "      <th>CW_stat</th>\n",
       "      <th>CW_p(one)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge(alpha=160)</td>\n",
       "      <td>0.215051</td>\n",
       "      <td>0.120751</td>\n",
       "      <td>0.4571</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>-0.020301</td>\n",
       "      <td>-35.153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.288</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model      RMSE       MAE  HitRatio  FitTime (s)    R2_OOS  \\\n",
       "0  Ridge(alpha=160)  0.215051  0.120751    0.4571       0.0788 -0.020301   \n",
       "\n",
       "   DM_stat  DM_p(two)  DM_p(one)  CW_stat  CW_p(one)  \n",
       "0  -35.153        0.0        1.0   -6.288        1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Ridge Regression ===\n",
    "\n",
    "# 1) Choose alpha via CV\n",
    "alphas = np.logspace(-4, 4, 50)  # from 1e-4 to 1e4\n",
    "ridge_cv = RidgeCV(alphas=alphas, store_cv_values=False)\n",
    "ridge_cv.fit(X_train_std, y_train)\n",
    "best_alpha = float(ridge_cv.alpha_)\n",
    "print(f\"[RidgeCV] best alpha = {best_alpha:.6g}\")\n",
    "\n",
    "# 4) Refit Ridge with best alpha\n",
    "ridge = Ridge(alpha=best_alpha, fit_intercept=True, random_state=42)\n",
    "ridge_results, ridge_model = evaluate_model(\n",
    "    ridge, X_train_std, y_train, X_test_std, y_test, model_name=f\"Ridge(alpha={best_alpha:.3g})\", hac_lag=0\n",
    ")\n",
    "\n",
    "print(\"=== Ridge Test Results ===\")\n",
    "display(ridge_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c5a84636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ridge Coefficients (standardized) ===\n",
      "        feature  coef_std\n",
      "         log_me -0.021790\n",
      "     mktrf_lag1  0.019399\n",
      "    size_log_at  0.015148\n",
      "            roa  0.011905\n",
      "       smb_lag1 -0.010277\n",
      " cash_to_assets  0.004110\n",
      "            lev -0.004038\n",
      "    sales_g_yoy  0.003756\n",
      "         vol_6m  0.003520\n",
      "             bm -0.002704\n",
      "       hml_lag1  0.001614\n",
      "    asset_g_yoy -0.001605\n",
      "capex_to_assets -0.001301\n",
      "       mom_12_2 -0.001261\n",
      "    sales_g_qoq  0.001248\n",
      "   gross_margin -0.001132\n",
      "        rf_lag1 -0.000714\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2) Print coefficients (standardized)\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"coef_std\": ridge_model.coef_\n",
    "}).sort_values(\"coef_std\", key=lambda s: s.abs(), ascending=False)\n",
    "\n",
    "print(\"\\n=== Ridge Coefficients (standardized) ===\")\n",
    "print(coef_df.to_string(index=False))\n",
    "\n",
    "# 3) Store results\n",
    "all_results.append(ridge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d7760d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ElasticNetCV] best alpha = 0.000325702, best l1_ratio = 0.1\n",
      "=== Elastic Net Test Results ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>HitRatio</th>\n",
       "      <th>FitTime (s)</th>\n",
       "      <th>R2_OOS</th>\n",
       "      <th>DM_stat</th>\n",
       "      <th>DM_p(two)</th>\n",
       "      <th>DM_p(one)</th>\n",
       "      <th>CW_stat</th>\n",
       "      <th>CW_p(one)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ElasticNet(alpha=0.000326, l1=0.1)</td>\n",
       "      <td>0.215037</td>\n",
       "      <td>0.120724</td>\n",
       "      <td>0.4568</td>\n",
       "      <td>0.467</td>\n",
       "      <td>-0.020169</td>\n",
       "      <td>-35.155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.432</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Model      RMSE       MAE  HitRatio  \\\n",
       "0  ElasticNet(alpha=0.000326, l1=0.1)  0.215037  0.120724    0.4568   \n",
       "\n",
       "   FitTime (s)    R2_OOS  DM_stat  DM_p(two)  DM_p(one)  CW_stat  CW_p(one)  \n",
       "0        0.467 -0.020169  -35.155        0.0        1.0   -6.432        1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Elastic Net Regression ===\n",
    "# 1) Hyperparameter search (CV) for alpha and l1_ratio\n",
    "alphas = np.logspace(-4, 1, 40)                 # 1e-4 … 10\n",
    "l1_grid = [0.1, 0.3, 0.5, 0.7, 0.9]             # blend between Ridge(0) and Lasso(1)\n",
    "enet_cv = ElasticNetCV(\n",
    "    alphas=alphas,\n",
    "    l1_ratio=l1_grid,\n",
    "    cv=5,\n",
    "    max_iter=10000,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    fit_intercept=True\n",
    ")\n",
    "enet_cv.fit(X_train_std, y_train)\n",
    "\n",
    "best_alpha = float(enet_cv.alpha_)\n",
    "best_l1    = float(enet_cv.l1_ratio_)\n",
    "print(f\"[ElasticNetCV] best alpha = {best_alpha:.6g}, best l1_ratio = {best_l1:.3g}\")\n",
    "\n",
    "# 2) Refit Elastic Net using the best params (for fair FitTime measurement)\n",
    "enet = ElasticNet(\n",
    "    alpha=best_alpha,\n",
    "    l1_ratio=best_l1,\n",
    "    max_iter=10000,\n",
    "    fit_intercept=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "enet_results, enet_model = evaluate_model(\n",
    "    enet, X_train_std, y_train, X_test_std, y_test,\n",
    "    model_name=f\"ElasticNet(alpha={best_alpha:.3g}, l1={best_l1:.2g})\"\n",
    ")\n",
    "\n",
    "print(\"=== Elastic Net Test Results ===\")\n",
    "display(enet_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bd0460d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonzero coefficients: 17 / 17\n",
      "\n",
      "=== Elastic Net Coefficients (standardized) ===\n",
      "        feature  coef_std\n",
      "         log_me -0.021486\n",
      "     mktrf_lag1  0.019350\n",
      "    size_log_at  0.014827\n",
      "            roa  0.011810\n",
      "       smb_lag1 -0.010225\n",
      " cash_to_assets  0.004027\n",
      "            lev -0.003910\n",
      "    sales_g_yoy  0.003698\n",
      "         vol_6m  0.003488\n",
      "             bm -0.002606\n",
      "       hml_lag1  0.001578\n",
      "    asset_g_yoy -0.001564\n",
      "capex_to_assets -0.001292\n",
      "       mom_12_2 -0.001244\n",
      "    sales_g_qoq  0.001232\n",
      "   gross_margin -0.001079\n",
      "        rf_lag1 -0.000694\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3) Coefficient inspection (standardized features)\n",
    "coef = enet_model.coef_\n",
    "nz_mask = coef != 0\n",
    "print(f\"Nonzero coefficients: {int(nz_mask.sum())} / {len(coef)}\")\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"coef_std\": coef\n",
    "}).sort_values(\"coef_std\", key=lambda s: s.abs(), ascending=False)\n",
    "\n",
    "print(\"\\n=== Elastic Net Coefficients (standardized) ===\")\n",
    "print(coef_df.to_string(index=False))\n",
    "\n",
    "# 4) Store results for later comparison\n",
    "all_results.append(enet_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab7c7ba",
   "metadata": {},
   "source": [
    "## Random Forest: Intuition and Application\n",
    "\n",
    "**Random Forest (RF)** is an ensemble learning method introduced by Breiman (2001).  \n",
    "It builds on decision trees but improves stability and accuracy through **bagging** and **random feature selection**.\n",
    "\n",
    "- **Decision Tree Basics**:  \n",
    "  Splits data recursively into regions based on predictor values.  \n",
    "  Simple and interpretable, but prone to **overfitting** and high variance.\n",
    "\n",
    "- **Random Forest Mechanics**:  \n",
    "  - **Bagging**: Each tree is trained on a bootstrap sample of the data.  \n",
    "  - **Feature Subsampling**: At each split, only a random subset of features is considered.  \n",
    "  - **Ensemble Prediction**:  \n",
    "    - Regression: average of predictions across trees.  \n",
    "    - Classification: majority vote across trees.  \n",
    "\n",
    "- **Strengths**:  \n",
    "  - Captures **nonlinearities** and **interactions** automatically.  \n",
    "  - Robust to noise and overfitting by averaging many decorrelated trees.  \n",
    "  - Scales well to high-dimensional datasets; training is parallelizable.  \n",
    "  - Provides **feature importance** measures useful in finance.\n",
    "\n",
    "- **Weaknesses**:  \n",
    "  - Acts like a **black box**; less interpretable than OLS or Lasso.  \n",
    "  - May be biased toward strong predictors if not tuned carefully.  \n",
    "  - Less efficient for sparse, very high-dimensional data.\n",
    "\n",
    "**Finance Takeaway**:  \n",
    "Random Forest is a strong off-the-shelf model for stock return prediction, especially when relationships are **nonlinear** and **noisy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8f1ca44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest Test Results ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>HitRatio</th>\n",
       "      <th>FitTime (s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForest(n=100)</td>\n",
       "      <td>0.216459</td>\n",
       "      <td>0.12147</td>\n",
       "      <td>0.4832</td>\n",
       "      <td>93.3928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model      RMSE      MAE  HitRatio  FitTime (s)\n",
       "0  RandomForest(n=100)  0.216459  0.12147    0.4832      93.3928"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1) Define RF model (sensible defaults for classroom demo)\n",
    "n_estimaters = 100\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=n_estimaters,\n",
    "    max_depth=None,          # let trees grow; adjust if overfitting\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',     # typical for tabular data\n",
    "    bootstrap=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2) Fit & evaluate\n",
    "rf_results, rf_model = evaluate_model(\n",
    "    rf, X_train, y_train, X_test, y_test, model_name=f\"RandomForest(n={n_estimaters})\"\n",
    ")\n",
    "\n",
    "print(\"=== Random Forest Test Results ===\")\n",
    "display(rf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "48a38a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Random Forest Feature Importances ===\n",
      "        feature  importance\n",
      "       mom_12_2    0.078764\n",
      "         vol_6m    0.072912\n",
      "         log_me    0.071178\n",
      "            roa    0.068506\n",
      "    asset_g_yoy    0.062817\n",
      "    size_log_at    0.060984\n",
      "    sales_g_yoy    0.060558\n",
      " cash_to_assets    0.059933\n",
      "   gross_margin    0.059080\n",
      "             bm    0.058632\n",
      "    sales_g_qoq    0.057720\n",
      "     mktrf_lag1    0.056624\n",
      "capex_to_assets    0.056414\n",
      "       hml_lag1    0.052144\n",
      "            lev    0.049331\n",
      "       smb_lag1    0.043170\n",
      "        rf_lag1    0.031232\n"
     ]
    }
   ],
   "source": [
    "# 3) Feature importances\n",
    "fi = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"importance\": rf_model.feature_importances_\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(\"\\n=== Random Forest Feature Importances ===\")\n",
    "print(fi.to_string(index=False))\n",
    "\n",
    "# 4) Store results\n",
    "all_results.append(rf_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501c84bb",
   "metadata": {},
   "source": [
    "## Gradient Boosting Variants: XGBoost and LightGBM\n",
    "\n",
    "**Gradient Boosting** builds trees sequentially, where each new tree corrects the errors of the previous ones.  \n",
    "Among its modern implementations, **XGBoost** and **LightGBM** are industry standards for structured/tabular data.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 XGBoost (Chen & Guestrin, 2016)\n",
    "- **Growth Strategy**: Level-wise (splits nodes layer by layer).\n",
    "- **Strengths**:\n",
    "  - Strong regularization (L1/L2 penalties) → helps prevent overfitting.\n",
    "  - Stable and widely used in Kaggle competitions and finance.\n",
    "  - Flexible with custom loss functions and objectives.\n",
    "- **Limitations**:\n",
    "  - Training can be slower on very large datasets compared to LightGBM.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 LightGBM (Ke et al., 2017)\n",
    "- **Growth Strategy**: Leaf-wise with depth constraints.\n",
    "- **Strengths**:\n",
    "  - Very **fast and memory-efficient** (histogram-based splitting).\n",
    "  - Handles large datasets and high-dimensional features effectively.\n",
    "  - Supports categorical variables natively.\n",
    "- **Limitations**:\n",
    "  - Leaf-wise growth may **overfit** small datasets if depth is not controlled.\n",
    "  - Requires careful tuning of learning rate, max depth, and regularization.\n",
    "\n",
    "---\n",
    "\n",
    "### Comparison\n",
    "| Feature               | XGBoost                     | LightGBM                       |\n",
    "|-----------------------|-----------------------------|--------------------------------|\n",
    "| Tree Growth           | Level-wise                  | Leaf-wise                      |\n",
    "| Speed & Memory        | Moderate                    | Very fast, memory-efficient    |\n",
    "| Regularization        | Strong (L1/L2)              | Relies more on depth control   |\n",
    "| Robustness            | Stable, widely adopted      | Faster but risk of overfitting |\n",
    "| Use Case in Finance   | Robust baseline             | Large-scale, high-frequency data|\n",
    "\n",
    "---\n",
    "\n",
    "**Finance Takeaway**:  \n",
    "Both are strong choices for **stock return prediction**.  \n",
    "- Use **XGBoost** for stability and interpretability.  \n",
    "- Use **LightGBM** when dataset is **large and high-dimensional** with speed requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5369ed09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== XGBoost Test Results ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>HitRatio</th>\n",
       "      <th>FitTime (s)</th>\n",
       "      <th>R2_OOS</th>\n",
       "      <th>DM_stat</th>\n",
       "      <th>DM_p(two)</th>\n",
       "      <th>DM_p(one)</th>\n",
       "      <th>CW_stat</th>\n",
       "      <th>CW_p(one)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost(500, lr=0.05)</td>\n",
       "      <td>0.221434</td>\n",
       "      <td>0.126543</td>\n",
       "      <td>0.4727</td>\n",
       "      <td>7.4757</td>\n",
       "      <td>-0.081771</td>\n",
       "      <td>-53.275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.336</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model      RMSE       MAE  HitRatio  FitTime (s)    R2_OOS  \\\n",
       "0  XGBoost(500, lr=0.05)  0.221434  0.126543    0.4727       7.4757 -0.081771   \n",
       "\n",
       "   DM_stat  DM_p(two)  DM_p(one)  CW_stat  CW_p(one)  \n",
       "0  -53.275        0.0        1.0   -5.336        1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === XGBoost Regression ===\n",
    "# 1) Define model (robust classroom defaults)\n",
    "xgb = XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,           # L2\n",
    "    reg_alpha=0.0,            # L1 (set >0 to encourage sparsity)\n",
    "    objective=\"reg:squarederror\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    tree_method=\"hist\"        # fast & memory-efficient\n",
    ")\n",
    "\n",
    "# 2) Fit & evaluate\n",
    "xgb_results, xgb_model = evaluate_model(\n",
    "    xgb, X_train, y_train, X_test, y_test, model_name=\"XGBoost(500, lr=0.05)\"\n",
    ")\n",
    "\n",
    "print(\"=== XGBoost Test Results ===\")\n",
    "display(xgb_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c5f5d96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== XGBoost Feature Importances ===\n",
      "        feature  importance\n",
      "        rf_lag1    0.135743\n",
      "       smb_lag1    0.126497\n",
      "     mktrf_lag1    0.115773\n",
      "       hml_lag1    0.088384\n",
      "       mom_12_2    0.061880\n",
      "            roa    0.060294\n",
      "         log_me    0.052715\n",
      "         vol_6m    0.045982\n",
      " cash_to_assets    0.038776\n",
      "    size_log_at    0.037453\n",
      "    asset_g_yoy    0.037017\n",
      "    sales_g_yoy    0.036372\n",
      "            lev    0.035543\n",
      "    sales_g_qoq    0.032711\n",
      "   gross_margin    0.031945\n",
      "             bm    0.031496\n",
      "capex_to_assets    0.031417\n"
     ]
    }
   ],
   "source": [
    "# 3) Feature importances\n",
    "xgb_fi = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"importance\": xgb_model.feature_importances_\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(\"\\n=== XGBoost Feature Importances ===\")\n",
    "print(xgb_fi.to_string(index=False))\n",
    "\n",
    "# 4) Store results\n",
    "all_results.append(xgb_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "573acfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021903 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3907\n",
      "[LightGBM] [Info] Number of data points in the train set: 838742, number of used features: 17\n",
      "[LightGBM] [Info] Start training from score 0.011211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\finnlp\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LightGBM Test Results ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>HitRatio</th>\n",
       "      <th>FitTime (s)</th>\n",
       "      <th>R2_OOS</th>\n",
       "      <th>DM_stat</th>\n",
       "      <th>DM_p(two)</th>\n",
       "      <th>DM_p(one)</th>\n",
       "      <th>CW_stat</th>\n",
       "      <th>CW_p(one)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM(600, lr=0.05)</td>\n",
       "      <td>0.218937</td>\n",
       "      <td>0.123706</td>\n",
       "      <td>0.4831</td>\n",
       "      <td>5.091</td>\n",
       "      <td>-0.057512</td>\n",
       "      <td>-36.222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>0.719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model      RMSE       MAE  HitRatio  FitTime (s)  \\\n",
       "0  LightGBM(600, lr=0.05)  0.218937  0.123706    0.4831        5.091   \n",
       "\n",
       "     R2_OOS  DM_stat  DM_p(two)  DM_p(one)  CW_stat  CW_p(one)  \n",
       "0 -0.057512  -36.222        0.0        1.0    -0.58      0.719  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === LightGBM Regression ===\n",
    "# 1) Define model (fast classroom defaults)\n",
    "lgbm = lgb.LGBMRegressor(\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    max_depth=-1,            # no explicit depth limit; controlled by num_leaves\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    reg_alpha=0.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 2) Fit & evaluate\n",
    "lgbm_results, lgbm_model = evaluate_model(\n",
    "    lgbm, X_train, y_train, X_test, y_test, model_name=\"LightGBM(600, lr=0.05)\"\n",
    ")\n",
    "\n",
    "print(\"=== LightGBM Test Results ===\")\n",
    "display(lgbm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "81586fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LightGBM Feature Importances (gain) ===\n",
      "        feature   importance\n",
      "     mktrf_lag1 12777.036388\n",
      "       smb_lag1 11786.200767\n",
      "       hml_lag1  9476.107720\n",
      "        rf_lag1  5676.009857\n",
      "       mom_12_2  4431.971649\n",
      "         log_me  3668.084253\n",
      "            roa  3247.086327\n",
      "         vol_6m  2908.542225\n",
      "    size_log_at  1605.264291\n",
      "    sales_g_yoy  1361.141584\n",
      "    asset_g_yoy  1347.752602\n",
      " cash_to_assets  1303.014182\n",
      "   gross_margin  1099.741701\n",
      "    sales_g_qoq  1069.474634\n",
      "             bm   926.490189\n",
      "            lev   862.132085\n",
      "capex_to_assets   850.146042\n"
     ]
    }
   ],
   "source": [
    "# 3) Feature importances (gain-based is often more informative than split-count)\n",
    "lgbm_fi = pd.DataFrame({\n",
    "    \"feature\": feature_cols,\n",
    "    \"importance\": lgbm_model.booster_.feature_importance(importance_type='gain')\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(\"\\n=== LightGBM Feature Importances (gain) ===\")\n",
    "print(lgbm_fi.to_string(index=False))\n",
    "\n",
    "# 4) Store results\n",
    "all_results.append(lgbm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ed5dc4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Summary of All Model Results ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>HitRatio</th>\n",
       "      <th>FitTime (s)</th>\n",
       "      <th>R2_OOS</th>\n",
       "      <th>DM_stat</th>\n",
       "      <th>DM_p(two)</th>\n",
       "      <th>DM_p(one)</th>\n",
       "      <th>CW_stat</th>\n",
       "      <th>CW_p(one)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OLS</td>\n",
       "      <td>0.215052</td>\n",
       "      <td>0.120753</td>\n",
       "      <td>0.4571</td>\n",
       "      <td>0.2757</td>\n",
       "      <td>-0.020313</td>\n",
       "      <td>-35.155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.278</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasso(alpha=0.0001)</td>\n",
       "      <td>0.215012</td>\n",
       "      <td>0.120678</td>\n",
       "      <td>0.4563</td>\n",
       "      <td>0.3938</td>\n",
       "      <td>-0.019935</td>\n",
       "      <td>-35.174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.710</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ridge(alpha=160)</td>\n",
       "      <td>0.215051</td>\n",
       "      <td>0.120751</td>\n",
       "      <td>0.4571</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>-0.020301</td>\n",
       "      <td>-35.153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.288</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost(500, lr=0.05)</td>\n",
       "      <td>0.221434</td>\n",
       "      <td>0.126543</td>\n",
       "      <td>0.4727</td>\n",
       "      <td>7.4757</td>\n",
       "      <td>-0.081771</td>\n",
       "      <td>-53.275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.336</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LightGBM(600, lr=0.05)</td>\n",
       "      <td>0.218937</td>\n",
       "      <td>0.123706</td>\n",
       "      <td>0.4831</td>\n",
       "      <td>5.0910</td>\n",
       "      <td>-0.057512</td>\n",
       "      <td>-36.222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.580</td>\n",
       "      <td>0.719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model      RMSE       MAE  HitRatio  FitTime (s)  \\\n",
       "0                     OLS  0.215052  0.120753    0.4571       0.2757   \n",
       "1     Lasso(alpha=0.0001)  0.215012  0.120678    0.4563       0.3938   \n",
       "2        Ridge(alpha=160)  0.215051  0.120751    0.4571       0.0788   \n",
       "3   XGBoost(500, lr=0.05)  0.221434  0.126543    0.4727       7.4757   \n",
       "4  LightGBM(600, lr=0.05)  0.218937  0.123706    0.4831       5.0910   \n",
       "\n",
       "     R2_OOS  DM_stat  DM_p(two)  DM_p(one)  CW_stat  CW_p(one)  \n",
       "0 -0.020313  -35.155        0.0        1.0   -6.278      1.000  \n",
       "1 -0.019935  -35.174        0.0        1.0   -6.710      1.000  \n",
       "2 -0.020301  -35.153        0.0        1.0   -6.288      1.000  \n",
       "3 -0.081771  -53.275        0.0        1.0   -5.336      1.000  \n",
       "4 -0.057512  -36.222        0.0        1.0   -0.580      0.719  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"=== Summary of All Model Results ===\")\n",
    "final_summary = pd.concat(all_results, ignore_index=True)\n",
    "display(final_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finnlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
